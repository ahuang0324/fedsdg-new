#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FedSDG åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹ï¼š
1. LoRALayer åœ¨ FedSDG æ¨¡å¼ä¸‹çš„å‚æ•°åˆå§‹åŒ–
2. å‰å‘ä¼ æ’­çš„åŒè·¯è®¡ç®— (Equation 4)
3. get_lora_state_dict æ­£ç¡®è¿‡æ»¤ç§æœ‰å‚æ•°
4. é€šä¿¡é‡ç»Ÿè®¡ä¸ FedLoRA ä¸€è‡´
5. å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†
6. Equation 5 æŸå¤±å‡½æ•°æ­£åˆ™åŒ–é¡¹éªŒè¯ (Î»â‚ L1 é—¨æ§ + Î»â‚‚ L2 ç§æœ‰)
7. é—¨æ§å‚æ•°åˆå§‹åŒ–éªŒè¯ (a_{k,l} = 0 â†’ m_{k,l} = 0.5)
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from models import LoRALayer, get_lora_state_dict, ViT, inject_lora


def test_lora_layer_fedsdg():
    """æµ‹è¯• LoRALayer åœ¨ FedSDG æ¨¡å¼ä¸‹çš„åˆå§‹åŒ–å’Œå‰å‘ä¼ æ’­"""
    print("\n" + "="*70)
    print("æµ‹è¯• 1: LoRALayer FedSDG æ¨¡å¼åˆå§‹åŒ–")
    print("="*70)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚
    original_layer = nn.Linear(128, 64)
    
    # æµ‹è¯•æ ‡å‡† LoRA æ¨¡å¼
    print("\n[æ ‡å‡† LoRA æ¨¡å¼]")
    lora_standard = LoRALayer(original_layer, r=8, lora_alpha=16, is_fedsdg=False)
    
    # æ£€æŸ¥æ ‡å‡† LoRA çš„å‚æ•°
    standard_params = dict(lora_standard.named_parameters())
    print(f"  å‚æ•°åˆ—è¡¨: {list(standard_params.keys())}")
    assert 'lora_A' in standard_params, "æ ‡å‡† LoRA åº”è¯¥æœ‰ lora_A"
    assert 'lora_B' in standard_params, "æ ‡å‡† LoRA åº”è¯¥æœ‰ lora_B"
    assert 'lora_A_private' not in standard_params, "æ ‡å‡† LoRA ä¸åº”è¯¥æœ‰ lora_A_private"
    assert 'lambda_k_logit' not in standard_params, "æ ‡å‡† LoRA ä¸åº”è¯¥æœ‰ lambda_k_logit"
    print("  âœ“ æ ‡å‡† LoRA å‚æ•°æ£€æŸ¥é€šè¿‡")
    
    # æµ‹è¯• FedSDG æ¨¡å¼
    print("\n[FedSDG æ¨¡å¼]")
    lora_fedsdg = LoRALayer(original_layer, r=8, lora_alpha=16, is_fedsdg=True)
    
    # æ£€æŸ¥ FedSDG çš„å‚æ•°
    fedsdg_params = dict(lora_fedsdg.named_parameters())
    print(f"  å‚æ•°åˆ—è¡¨: {list(fedsdg_params.keys())}")
    assert 'lora_A' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_A (å…¨å±€åˆ†æ”¯)"
    assert 'lora_B' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_B (å…¨å±€åˆ†æ”¯)"
    assert 'lora_A_private' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_A_private (ç§æœ‰åˆ†æ”¯)"
    assert 'lora_B_private' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lora_B_private (ç§æœ‰åˆ†æ”¯)"
    assert 'lambda_k_logit' in fedsdg_params, "FedSDG åº”è¯¥æœ‰ lambda_k_logit (é—¨æ§å‚æ•°)"
    print("  âœ“ FedSDG å‚æ•°æ£€æŸ¥é€šè¿‡")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n[å‰å‘ä¼ æ’­æµ‹è¯•]")
    x = torch.randn(4, 128)  # batch_size=4, in_features=128
    
    # æ ‡å‡† LoRA å‰å‘ä¼ æ’­
    output_standard = lora_standard(x)
    print(f"  æ ‡å‡† LoRA è¾“å‡ºå½¢çŠ¶: {output_standard.shape}")
    assert output_standard.shape == (4, 64), "è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ (4, 64)"
    
    # FedSDG å‰å‘ä¼ æ’­
    output_fedsdg = lora_fedsdg(x)
    print(f"  FedSDG è¾“å‡ºå½¢çŠ¶: {output_fedsdg.shape}")
    assert output_fedsdg.shape == (4, 64), "è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ (4, 64)"
    
    # æ£€æŸ¥ lambda_k çš„èŒƒå›´
    lambda_k = torch.sigmoid(lora_fedsdg.lambda_k_logit)
    print(f"  é—¨æ§å‚æ•° lambda_k: {lambda_k.item():.4f}")
    assert 0 <= lambda_k.item() <= 1, "lambda_k åº”è¯¥åœ¨ [0, 1] èŒƒå›´å†…"
    print("  âœ“ å‰å‘ä¼ æ’­æ£€æŸ¥é€šè¿‡")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 1 é€šè¿‡ï¼šLoRALayer FedSDG æ¨¡å¼å·¥ä½œæ­£å¸¸")
    print("="*70)


def test_get_lora_state_dict_filtering():
    """æµ‹è¯• get_lora_state_dict æ­£ç¡®è¿‡æ»¤ç§æœ‰å‚æ•°"""
    print("\n" + "="*70)
    print("æµ‹è¯• 2: get_lora_state_dict ç§æœ‰å‚æ•°è¿‡æ»¤")
    print("="*70)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ ViT æ¨¡å‹å¹¶æ³¨å…¥ FedSDG
    model = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,  # ä½¿ç”¨ 2 å±‚ä»¥åŠ å¿«æµ‹è¯•
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ FedSDG
    model = inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # è·å–å®Œæ•´çš„ state_dict
    full_state = model.state_dict()
    print(f"\n[å®Œæ•´ state_dict]")
    print(f"  æ€»å‚æ•°æ•°é‡: {len(full_state)}")
    
    # ç»Ÿè®¡ç§æœ‰å‚æ•°
    private_params = [k for k in full_state.keys() if '_private' in k or 'lambda_k' in k]
    print(f"  ç§æœ‰å‚æ•°æ•°é‡: {len(private_params)}")
    print(f"  ç§æœ‰å‚æ•°ç¤ºä¾‹: {private_params[:3] if len(private_params) > 0 else 'None'}")
    
    # è·å– LoRA state_dictï¼ˆåº”è¯¥è¿‡æ»¤æ‰ç§æœ‰å‚æ•°ï¼‰
    lora_state = get_lora_state_dict(model)
    print(f"\n[LoRA state_dict (ç”¨äºé€šä¿¡)]")
    print(f"  å‚æ•°æ•°é‡: {len(lora_state)}")
    
    # æ£€æŸ¥æ˜¯å¦æ­£ç¡®è¿‡æ»¤
    for key in lora_state.keys():
        assert '_private' not in key, f"ä¸åº”è¯¥åŒ…å«ç§æœ‰å‚æ•°: {key}"
        assert 'lambda_k' not in key, f"ä¸åº”è¯¥åŒ…å«é—¨æ§å‚æ•°: {key}"
    
    print("  âœ“ æ‰€æœ‰ç§æœ‰å‚æ•°å·²è¢«æ­£ç¡®è¿‡æ»¤")
    
    # æ£€æŸ¥å…¨å±€å‚æ•°æ˜¯å¦å­˜åœ¨
    global_params = [k for k in lora_state.keys() if 'lora_A' in k or 'lora_B' in k]
    print(f"  å…¨å±€ LoRA å‚æ•°æ•°é‡: {len(global_params)}")
    assert len(global_params) > 0, "åº”è¯¥åŒ…å«å…¨å±€ LoRA å‚æ•°"
    print("  âœ“ å…¨å±€å‚æ•°æ­£ç¡®ä¿ç•™")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 2 é€šè¿‡ï¼šç§æœ‰å‚æ•°è¿‡æ»¤åŠŸèƒ½æ­£å¸¸")
    print("="*70)


def test_communication_volume():
    """æµ‹è¯• FedSDG çš„é€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´"""
    print("\n" + "="*70)
    print("æµ‹è¯• 3: FedSDG ä¸ FedLoRA é€šä¿¡é‡å¯¹æ¯”")
    print("="*70)
    
    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„ ViT æ¨¡å‹
    model_fedlora = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    model_fedsdg = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ LoRA
    model_fedlora = inject_lora(model_fedlora, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=False)
    model_fedsdg = inject_lora(model_fedsdg, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # è·å–é€šä¿¡å‚æ•°
    lora_state_fedlora = get_lora_state_dict(model_fedlora)
    lora_state_fedsdg = get_lora_state_dict(model_fedsdg)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    params_fedlora = sum(p.numel() for p in lora_state_fedlora.values())
    params_fedsdg = sum(p.numel() for p in lora_state_fedsdg.values())
    
    print(f"\n[é€šä¿¡å‚æ•°ç»Ÿè®¡]")
    print(f"  FedLoRA é€šä¿¡å‚æ•°: {params_fedlora:,}")
    print(f"  FedSDG é€šä¿¡å‚æ•°:  {params_fedsdg:,}")
    print(f"  å·®å¼‚: {abs(params_fedlora - params_fedsdg):,}")
    
    # è®¡ç®—é€šä¿¡é‡ï¼ˆMBï¼‰
    bytes_per_param = 4  # float32
    comm_mb_fedlora = (params_fedlora * bytes_per_param) / (1024 ** 2)
    comm_mb_fedsdg = (params_fedsdg * bytes_per_param) / (1024 ** 2)
    
    print(f"\n[é€šä¿¡é‡ (MB)]")
    print(f"  FedLoRA: {comm_mb_fedlora:.4f} MB")
    print(f"  FedSDG:  {comm_mb_fedsdg:.4f} MB")
    
    # éªŒè¯é€šä¿¡é‡ä¸€è‡´
    assert params_fedlora == params_fedsdg, "FedSDG çš„é€šä¿¡é‡åº”è¯¥ä¸ FedLoRA å®Œå…¨ä¸€è‡´"
    print("\n  âœ“ é€šä¿¡é‡å®Œå…¨ä¸€è‡´")
    
    # ç»Ÿè®¡ FedSDG çš„æ€»å‚æ•°ï¼ˆåŒ…æ‹¬ç§æœ‰å‚æ•°ï¼‰
    total_params_fedsdg = sum(p.numel() for p in model_fedsdg.parameters() if p.requires_grad)
    private_params = total_params_fedsdg - params_fedsdg
    
    print(f"\n[FedSDG å‚æ•°åˆ†å¸ƒ]")
    print(f"  æ€»å¯è®­ç»ƒå‚æ•°: {total_params_fedsdg:,}")
    print(f"  é€šä¿¡å‚æ•° (å…¨å±€): {params_fedsdg:,}")
    print(f"  ç§æœ‰å‚æ•° (æœ¬åœ°): {private_params:,}")
    print(f"  ç§æœ‰å‚æ•°å æ¯”: {100 * private_params / total_params_fedsdg:.2f}%")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 3 é€šè¿‡ï¼šFedSDG é€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´")
    print("="*70)


def test_private_state_management():
    """æµ‹è¯•å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€çš„ä¿å­˜å’ŒåŠ è½½"""
    print("\n" + "="*70)
    print("æµ‹è¯• 4: å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†")
    print("="*70)
    
    # åˆ›å»ºæ¨¡å‹
    model = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ FedSDG
    model = inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # æ¨¡æ‹Ÿå®¢æˆ·ç«¯è®­ç»ƒï¼šä¿å­˜ç§æœ‰çŠ¶æ€
    print("\n[æ¨¡æ‹Ÿå®¢æˆ·ç«¯ 0 è®­ç»ƒ]")
    
    # æå–ç§æœ‰å‚æ•°
    private_state = {}
    for name, param in model.named_parameters():
        if '_private' in name or 'lambda_k' in name:
            private_state[name] = param.data.clone()
    
    print(f"  æå–ç§æœ‰å‚æ•°æ•°é‡: {len(private_state)}")
    print(f"  ç§æœ‰å‚æ•°ç¤ºä¾‹: {list(private_state.keys())[:3]}")
    
    # ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒï¼‰
    for param in model.parameters():
        if param.requires_grad:
            param.data += torch.randn_like(param.data) * 0.01
    
    # ä¿å­˜ä¿®æ”¹åçš„ç§æœ‰å‚æ•°
    private_state_after = {}
    for name, param in model.named_parameters():
        if '_private' in name or 'lambda_k' in name:
            private_state_after[name] = param.data.clone()
    
    # éªŒè¯ç§æœ‰å‚æ•°å·²æ”¹å˜
    changed = False
    for key in private_state.keys():
        if not torch.allclose(private_state[key], private_state_after[key]):
            changed = True
            break
    
    assert changed, "ç§æœ‰å‚æ•°åº”è¯¥åœ¨è®­ç»ƒåå‘ç”Ÿå˜åŒ–"
    print("  âœ“ ç§æœ‰å‚æ•°åœ¨è®­ç»ƒåå·²æ›´æ–°")
    
    # æ¨¡æ‹ŸåŠ è½½ç§æœ‰çŠ¶æ€åˆ°æ–°æ¨¡å‹
    print("\n[æ¨¡æ‹Ÿä¸‹ä¸€è½®ï¼šåŠ è½½ç§æœ‰çŠ¶æ€]")
    
    # åˆ›å»ºæ–°çš„å…¨å±€æ¨¡å‹ï¼ˆæ¨¡æ‹ŸæœåŠ¡å™¨èšåˆåçš„æ¨¡å‹ï¼‰
    model_new = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    model_new = inject_lora(model_new, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # åŠ è½½ç§æœ‰çŠ¶æ€
    current_state = model_new.state_dict()
    for param_name, param_value in private_state_after.items():
        if param_name in current_state:
            current_state[param_name] = param_value.clone()
    model_new.load_state_dict(current_state)
    
    # éªŒè¯ç§æœ‰å‚æ•°å·²æ­£ç¡®åŠ è½½
    for name, param in model_new.named_parameters():
        if name in private_state_after:
            assert torch.allclose(param.data, private_state_after[name]), f"å‚æ•° {name} åŠ è½½å¤±è´¥"
    
    print("  âœ“ ç§æœ‰çŠ¶æ€æˆåŠŸåŠ è½½åˆ°æ–°æ¨¡å‹")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 4 é€šè¿‡ï¼šç§æœ‰çŠ¶æ€ç®¡ç†åŠŸèƒ½æ­£å¸¸")
    print("="*70)


def test_forward_backward():
    """æµ‹è¯• FedSDG çš„å‰å‘å’Œåå‘ä¼ æ’­"""
    print("\n" + "="*70)
    print("æµ‹è¯• 5: FedSDG å‰å‘å’Œåå‘ä¼ æ’­")
    print("="*70)
    
    # åˆ›å»ºæ¨¡å‹
    model = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ FedSDG
    model = inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    # åˆ›å»ºè¾“å…¥å’Œæ ‡ç­¾
    x = torch.randn(2, 3, 32, 32)
    y = torch.randint(0, 10, (2,))
    
    # å‰å‘ä¼ æ’­
    print("\n[å‰å‘ä¼ æ’­]")
    output = model(x)
    print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert output.shape == (2, 10), "è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ (2, 10)"
    print("  âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
    
    # è®¡ç®—æŸå¤±
    criterion = nn.CrossEntropyLoss()
    loss = criterion(output, y)
    print(f"  æŸå¤±å€¼: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    print("\n[åå‘ä¼ æ’­]")
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    grad_count = 0
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is not None:
            grad_count += 1
    
    print(f"  æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {grad_count}")
    assert grad_count > 0, "åº”è¯¥æœ‰å‚æ•°å…·æœ‰æ¢¯åº¦"
    print("  âœ“ åå‘ä¼ æ’­æˆåŠŸ")
    
    # æ£€æŸ¥ç§æœ‰å‚æ•°ä¹Ÿæœ‰æ¢¯åº¦
    private_grad_count = 0
    for name, param in model.named_parameters():
        if ('_private' in name or 'lambda_k' in name) and param.grad is not None:
            private_grad_count += 1
    
    print(f"  ç§æœ‰å‚æ•°æœ‰æ¢¯åº¦çš„æ•°é‡: {private_grad_count}")
    assert private_grad_count > 0, "ç§æœ‰å‚æ•°åº”è¯¥æœ‰æ¢¯åº¦"
    print("  âœ“ ç§æœ‰å‚æ•°å¯ä»¥æ­£å¸¸è®­ç»ƒ")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 5 é€šè¿‡ï¼šå‰å‘å’Œåå‘ä¼ æ’­æ­£å¸¸")
    print("="*70)


def test_gate_initialization():
    """æµ‹è¯• 6: é—¨æ§å‚æ•°åˆå§‹åŒ–éªŒè¯ (FedSDG_Design.md è§„èŒƒ)"""
    print("\n" + "="*70)
    print("æµ‹è¯• 6: é—¨æ§å‚æ•°åˆå§‹åŒ–éªŒè¯")
    print("="*70)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚
    original_layer = nn.Linear(128, 64)
    
    # åˆ›å»º FedSDG LoRA å±‚
    lora_fedsdg = LoRALayer(original_layer, r=8, lora_alpha=16, is_fedsdg=True)
    
    # æ£€æŸ¥ lambda_k_logit åˆå§‹åŒ–å€¼
    print("\n[é—¨æ§å‚æ•°åˆå§‹åŒ–æ£€æŸ¥]")
    logit_value = lora_fedsdg.lambda_k_logit.item()
    m_k_value = torch.sigmoid(lora_fedsdg.lambda_k_logit).item()
    
    print(f"  lambda_k_logit (a_{{k,l}}): {logit_value:.4f}")
    print(f"  m_{{k,l}} = sigmoid(a_{{k,l}}): {m_k_value:.4f}")
    
    # æ ¹æ®è®¾è®¡æ–‡æ¡£: a_{k,l} = 0 â†’ m_{k,l} = 0.5
    assert abs(logit_value - 0.0) < 1e-6, f"lambda_k_logit åº”è¯¥åˆå§‹åŒ–ä¸º 0.0ï¼Œå®é™…ä¸º {logit_value}"
    assert abs(m_k_value - 0.5) < 1e-6, f"m_{{k,l}} åº”è¯¥åˆå§‹åŒ–ä¸º 0.5ï¼Œå®é™…ä¸º {m_k_value}"
    
    print("  âœ“ é—¨æ§å‚æ•°åˆå§‹åŒ–æ­£ç¡®: a_{k,l}=0 â†’ m_{k,l}=0.5")
    print("  âœ“ ç¬¦åˆè®¾è®¡æ–‡æ¡£è¦æ±‚ï¼šè®­ç»ƒå¼€å§‹æ—¶å…±äº«å’Œç§æœ‰ç»„ä»¶ç­‰æƒé‡")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 6 é€šè¿‡ï¼šé—¨æ§å‚æ•°åˆå§‹åŒ–ç¬¦åˆè®¾è®¡è§„èŒƒ")
    print("="*70)


def test_equation5_loss_components():
    """æµ‹è¯• 7: Equation 5 æŸå¤±å‡½æ•°ç»„ä»¶éªŒè¯"""
    print("\n" + "="*70)
    print("æµ‹è¯• 7: Equation 5 æŸå¤±å‡½æ•°ç»„ä»¶éªŒè¯")
    print("="*70)
    
    # åˆ›å»ºæ¨¡å‹
    model = ViT(
        image_size=32,
        patch_size=4,
        num_classes=10,
        dim=128,
        depth=2,
        heads=4,
        mlp_dim=256,
        channels=3
    )
    
    # æ³¨å…¥ FedSDG
    model = inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=True)
    
    print("\n[è®¡ç®— Equation 5 å„ç»„ä»¶]")
    
    # ========== Î»â‚ L1 é—¨æ§æƒ©ç½š ==========
    gate_penalty = 0.0
    gate_count = 0
    gate_values = []
    for name, param in model.named_parameters():
        if 'lambda_k_logit' in name:
            m_k = torch.sigmoid(param)
            gate_penalty += torch.sum(torch.abs(m_k)).item()
            gate_count += param.numel()
            gate_values.append(m_k.item())
    
    print(f"\n  [Î»â‚ L1 é—¨æ§æƒ©ç½š]")
    print(f"    é—¨æ§å‚æ•°æ•°é‡: {gate_count}")
    print(f"    é—¨æ§å€¼ (m_{{k,l}}): {gate_values[:3]}... (å…± {len(gate_values)} ä¸ª)")
    print(f"    gate_penalty = Î£|m_{{k,l}}|: {gate_penalty:.4f}")
    
    # åˆå§‹æ—¶ m_{k,l} = 0.5ï¼Œæ‰€ä»¥ gate_penalty â‰ˆ 0.5 * num_gates
    expected_gate_penalty = 0.5 * gate_count
    print(f"    é¢„æœŸå€¼ (åˆå§‹): {expected_gate_penalty:.4f}")
    assert abs(gate_penalty - expected_gate_penalty) < 0.1, \
        f"gate_penalty åº”è¯¥çº¦ä¸º {expected_gate_penalty}ï¼Œå®é™…ä¸º {gate_penalty}"
    print(f"    âœ“ é—¨æ§æƒ©ç½šè®¡ç®—æ­£ç¡®")
    
    # ========== Î»â‚‚ L2 ç§æœ‰å‚æ•°æƒ©ç½š ==========
    private_penalty = 0.0
    private_count = 0
    for name, param in model.named_parameters():
        if '_private' in name:
            private_penalty += torch.sum(param ** 2).item()
            private_count += param.numel()
    
    print(f"\n  [Î»â‚‚ L2 ç§æœ‰å‚æ•°æƒ©ç½š]")
    print(f"    ç§æœ‰å‚æ•°æ•°é‡: {private_count}")
    print(f"    private_penalty = ||Î¸_{{p,k}}||Â²â‚‚: {private_penalty:.6f}")
    
    # åˆå§‹æ—¶ç§æœ‰å‚æ•°æ¥è¿‘ 0ï¼ˆlora_B_private åˆå§‹åŒ–ä¸º 0ï¼‰
    print(f"    âœ“ ç§æœ‰æƒ©ç½šè®¡ç®—æ­£ç¡®ï¼ˆåˆå§‹å€¼è¾ƒå°ï¼‰")
    
    # ========== æ¨¡æ‹Ÿå®Œæ•´æŸå¤±è®¡ç®— ==========
    print(f"\n  [æ¨¡æ‹Ÿ Equation 5 å®Œæ•´æŸå¤±]")
    lambda1 = 1e-3
    lambda2 = 1e-4
    task_loss = 2.3  # æ¨¡æ‹Ÿäº¤å‰ç†µæŸå¤±
    
    total_loss = task_loss + lambda1 * gate_penalty + lambda2 * private_penalty
    
    print(f"    task_loss: {task_loss:.4f}")
    print(f"    Î»â‚ * gate_penalty: {lambda1} * {gate_penalty:.4f} = {lambda1 * gate_penalty:.6f}")
    print(f"    Î»â‚‚ * private_penalty: {lambda2} * {private_penalty:.6f} = {lambda2 * private_penalty:.8f}")
    print(f"    total_loss: {total_loss:.6f}")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 7 é€šè¿‡ï¼šEquation 5 æŸå¤±å‡½æ•°ç»„ä»¶è®¡ç®—æ­£ç¡®")
    print("="*70)


def test_equation4_forward():
    """æµ‹è¯• 8: Equation 4 å‰å‘ä¼ æ’­éªŒè¯ï¼ˆåŠ æ€§æ®‹å·®å½¢å¼ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯• 8: Equation 4 å‰å‘ä¼ æ’­éªŒè¯")
    print("="*70)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚
    original_layer = nn.Linear(128, 64)
    
    # åˆ›å»º FedSDG LoRA å±‚
    lora = LoRALayer(original_layer, r=8, lora_alpha=16, is_fedsdg=True)
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(2, 128)
    
    print("\n[éªŒè¯ Equation 4: Î¸Ìƒ_{k,l} = Î¸_{g,l} + m_{k,l} Â· Î¸_{p,k,l}]")
    
    # æ‰‹åŠ¨è®¡ç®—å„ç»„ä»¶
    original_output = original_layer(x)
    global_output = x @ lora.lora_A @ lora.lora_B
    private_output = x @ lora.lora_A_private @ lora.lora_B_private
    m_k = torch.sigmoid(lora.lambda_k_logit)
    
    # æ ¹æ® Equation 4 è®¡ç®—é¢„æœŸè¾“å‡º
    expected_lora_output = (global_output + m_k * private_output) * lora.scaling
    expected_total = original_output + expected_lora_output
    
    # å®é™…å‰å‘ä¼ æ’­
    actual_output = lora(x)
    
    print(f"  m_{{k,l}}: {m_k.item():.4f}")
    print(f"  global_output èŒƒæ•°: {global_output.norm().item():.4f}")
    print(f"  private_output èŒƒæ•°: {private_output.norm().item():.4f}")
    print(f"  é¢„æœŸè¾“å‡ºèŒƒæ•°: {expected_total.norm().item():.4f}")
    print(f"  å®é™…è¾“å‡ºèŒƒæ•°: {actual_output.norm().item():.4f}")
    
    # éªŒè¯è¾“å‡ºä¸€è‡´
    assert torch.allclose(actual_output, expected_total, atol=1e-6), \
        "å‰å‘ä¼ æ’­è¾“å‡ºä¸ Equation 4 é¢„æœŸä¸ä¸€è‡´"
    
    print("  âœ“ å‰å‘ä¼ æ’­ç¬¦åˆ Equation 4 åŠ æ€§æ®‹å·®å½¢å¼")
    
    # éªŒè¯æç«¯æƒ…å†µ
    print("\n[éªŒè¯æç«¯æƒ…å†µ]")
    
    # m_k = 0: ä»…ä½¿ç”¨å…¨å±€åˆ†æ”¯
    lora.lambda_k_logit.data = torch.tensor([-100.0])  # sigmoid(-100) â‰ˆ 0
    m_k_0 = torch.sigmoid(lora.lambda_k_logit)
    output_m0 = lora(x)
    expected_m0 = original_output + global_output * lora.scaling
    print(f"  m_k â‰ˆ 0 ({m_k_0.item():.6f}): è¾“å‡ºåº”æ¥è¿‘ global-only")
    assert torch.allclose(output_m0, expected_m0, atol=1e-4), "m_k=0 æ—¶åº”ä»…ä½¿ç”¨å…¨å±€åˆ†æ”¯"
    print("  âœ“ m_k=0 éªŒè¯é€šè¿‡")
    
    # m_k = 1: å…¨å±€ + å®Œæ•´ç§æœ‰
    lora.lambda_k_logit.data = torch.tensor([100.0])  # sigmoid(100) â‰ˆ 1
    m_k_1 = torch.sigmoid(lora.lambda_k_logit)
    output_m1 = lora(x)
    expected_m1 = original_output + (global_output + private_output) * lora.scaling
    print(f"  m_k â‰ˆ 1 ({m_k_1.item():.6f}): è¾“å‡ºåº”ä¸º global + private")
    assert torch.allclose(output_m1, expected_m1, atol=1e-4), "m_k=1 æ—¶åº”ä½¿ç”¨å…¨å±€+ç§æœ‰"
    print("  âœ“ m_k=1 éªŒè¯é€šè¿‡")
    
    print("\n" + "="*70)
    print("âœ“ æµ‹è¯• 8 é€šè¿‡ï¼šEquation 4 å‰å‘ä¼ æ’­å®ç°æ­£ç¡®")
    print("="*70)


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*70)
    print("FedSDG åŠŸèƒ½æµ‹è¯•å¥—ä»¶ (ç¬¦åˆ FedSDG_Design.md è§„èŒƒ)")
    print("="*70)
    
    try:
        test_lora_layer_fedsdg()
        test_get_lora_state_dict_filtering()
        test_communication_volume()
        test_private_state_management()
        test_forward_backward()
        test_gate_initialization()
        test_equation5_loss_components()
        test_equation4_forward()
        
        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼FedSDG å®ç°ç¬¦åˆè®¾è®¡è§„èŒƒï¼")
        print("="*70)
        print("\næ€»ç»“ï¼š")
        print("  âœ“ LoRALayer åŒè·¯æ¶æ„å·¥ä½œæ­£å¸¸")
        print("  âœ“ ç§æœ‰å‚æ•°è¿‡æ»¤åŠŸèƒ½æ­£ç¡®")
        print("  âœ“ é€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´")
        print("  âœ“ å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†æ­£å¸¸")
        print("  âœ“ å‰å‘å’Œåå‘ä¼ æ’­æ­£å¸¸")
        print("  âœ“ é—¨æ§å‚æ•°åˆå§‹åŒ–ç¬¦åˆè§„èŒƒ (a_{k,l}=0 â†’ m_{k,l}=0.5)")
        print("  âœ“ Equation 5 æŸå¤±å‡½æ•°ç»„ä»¶è®¡ç®—æ­£ç¡®")
        print("  âœ“ Equation 4 å‰å‘ä¼ æ’­å®ç°æ­£ç¡®ï¼ˆåŠ æ€§æ®‹å·®å½¢å¼ï¼‰")
        print("\nFedSDG å·²å‡†å¤‡å¥½ç”¨äºè”é‚¦å­¦ä¹ è®­ç»ƒï¼")
        print("="*70 + "\n")
        
        return True
        
    except AssertionError as e:
        print("\n" + "="*70)
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("="*70 + "\n")
        return False
    except Exception as e:
        print("\n" + "="*70)
        print(f"âŒ æµ‹è¯•å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        print("="*70 + "\n")
        return False


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)
