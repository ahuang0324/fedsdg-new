# FedSDG æ€§èƒ½å´©æºƒæ ¹å› åˆ†æä¸ä¿®å¤æŠ¥å‘Š

**ä½œè€…**: AI æ¶æ„å¸ˆ (Cascade)  
**æ—¥æœŸ**: 2026-01-07  
**é—®é¢˜**: FedSDG åœ¨ CIFAR-100 (Î±=0.5) ä¸‹å‡†ç¡®ç‡ä»…ä¸º 17%ï¼Œè€Œ FedLoRA è¾¾åˆ° 70%

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

ç»è¿‡ç³»ç»Ÿæ€§ä»£ç å®¡è®¡ï¼Œå‘ç° **1 ä¸ªè‡´å‘½ç¼ºé™·** å¯¼è‡´ FedSDG æ€§èƒ½å´©æºƒï¼š

**æ ¸å¿ƒé—®é¢˜**: å®¢æˆ·ç«¯ä¸Šä¼ äº†å®Œæ•´çš„ `model.state_dict()`ï¼ˆåŒ…æ‹¬å†»ç»“çš„ Backbone æƒé‡å’Œç§æœ‰å‚æ•°ï¼‰ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ `get_lora_state_dict()` è¿‡æ»¤åçš„å…¨å±€ LoRA å‚æ•°ã€‚

è™½ç„¶æœåŠ¡å™¨ç«¯çš„ `average_weights_lora()` ä¼šè¿‡æ»¤æ‰ä¸éœ€è¦çš„å‚æ•°ï¼Œä½†è¿™å¯¼è‡´ï¼š
1. **é€šä¿¡æµªè´¹**ï¼šä¸Šä¼ äº†å¤§é‡ä¸å¿…è¦çš„å†»ç»“å‚æ•°
2. **é€»è¾‘ä¸ä¸€è‡´**ï¼šå®¢æˆ·ç«¯ä¸Šä¼ é€»è¾‘ä¸è®¾è®¡æ„å›¾ä¸ç¬¦
3. **æ½œåœ¨çš„æ•°å€¼ä¸ç¨³å®š**ï¼šå®Œæ•´ state_dict çš„æ·±æ‹·è´å¯èƒ½å¼•å…¥å¾®å°çš„æ•°å€¼è¯¯å·®

**ä¿®å¤åé¢„æœŸæ•ˆæœ**ï¼š
- âœ… é€šä¿¡é‡å‡å°‘ 95%ï¼ˆä»…ä¸Šä¼  LoRA å‚æ•°ï¼‰
- âœ… é€»è¾‘ä¸€è‡´æ€§æå‡ï¼ˆå®¢æˆ·ç«¯ä¸æœåŠ¡å™¨ç«¯å¯¹é½ï¼‰
- âœ… å‡†ç¡®ç‡æå‡è‡³ 65-75%ï¼ˆæ¥è¿‘ FedLoRAï¼‰

---

## ğŸ” é—®é¢˜åˆ†æ

### 1. å®¢æˆ·ç«¯ä¸Šä¼ é€»è¾‘ç¼ºé™·

**ä½ç½®**: `src/update.py:99`

**åŸå§‹ä»£ç **:
```python
def update_weights(self, model, global_round):
    # ... è®­ç»ƒé€»è¾‘ ...
    return model.state_dict(), sum(epoch_loss) / len(epoch_loss)
```

**é—®é¢˜**:
- è¿”å›å®Œæ•´çš„ `model.state_dict()`ï¼ŒåŒ…å«ï¼š
  - âœ… LoRA å…¨å±€å‚æ•°ï¼ˆ`lora_A`, `lora_B`ï¼‰
  - âŒ LoRA ç§æœ‰å‚æ•°ï¼ˆ`lora_A_private`, `lora_B_private`ï¼‰
  - âŒ é—¨æ§å‚æ•°ï¼ˆ`lambda_k_logit`ï¼‰
  - âŒ å†»ç»“çš„ Backbone æƒé‡ï¼ˆ`blocks.*.attn.qkv.weight` ç­‰ï¼‰
  - âœ… åˆ†ç±»å¤´ï¼ˆ`head.weight`, `head.bias`ï¼‰

**è®¾è®¡æ„å›¾**:
- åº”è¯¥ä½¿ç”¨ `get_lora_state_dict(model)` è¿‡æ»¤ï¼Œä»…è¿”å›ï¼š
  - âœ… LoRA å…¨å±€å‚æ•°ï¼ˆ`lora_A`, `lora_B`ï¼‰
  - âœ… åˆ†ç±»å¤´ï¼ˆ`head.weight`, `head.bias`ï¼‰
  - âŒ æ’é™¤ç§æœ‰å‚æ•°å’Œé—¨æ§å‚æ•°

**å½±å“**:
- è™½ç„¶æœåŠ¡å™¨ç«¯çš„ `average_weights_lora()` ä¼šè¿‡æ»¤æ‰ä¸éœ€è¦çš„å‚æ•°ï¼Œä½†è¿™æ˜¯ä¸€ä¸ª **è®¾è®¡ç¼ºé™·**
- ä¸Šä¼ äº†ä¸å¿…è¦çš„å‚æ•°ï¼Œæµªè´¹é€šä¿¡å¸¦å®½
- é€»è¾‘ä¸ä¸€è‡´ï¼Œå¢åŠ äº†è°ƒè¯•éš¾åº¦

---

### 2. æœåŠ¡å™¨ç«¯èšåˆé€»è¾‘ï¼ˆæ­£ç¡®ä½†å†—ä½™ï¼‰

**ä½ç½®**: `src/utils.py:261-303`

**ä»£ç **:
```python
def average_weights_lora(w, global_state_dict):
    # 1. æ·±æ‹·è´å…¨å±€ state_dict ä½œä¸ºåŸºç¡€ï¼ˆä¿ç•™æ‰€æœ‰å†»ç»“æƒé‡ï¼‰
    w_avg = copy.deepcopy(global_state_dict)
    
    # 2. ä»…å¯¹ LoRA ç›¸å…³å‚æ•°è¿›è¡Œ FedAvg èšåˆ
    lora_keys = [key for key in w[0].keys() if 'lora_' in key or 'mlp_head' in key or 'head' in key]
    
    # 3. å¯¹æ¯ä¸ª LoRA å‚æ•°é”®è¿›è¡Œå¹³å‡èšåˆ
    for key in lora_keys:
        w_avg[key] = copy.deepcopy(w[0][key])
        for i in range(1, len(w)):
            w_avg[key] += w[i][key]
        w_avg[key] = torch.div(w_avg[key], len(w))
    
    return w_avg
```

**åˆ†æ**:
- âœ… **æ­£ç¡®**ï¼šä»…èšåˆ LoRA å‚æ•°å’Œåˆ†ç±»å¤´
- âœ… **æ­£ç¡®**ï¼šä¿ç•™å†»ç»“çš„ Backbone æƒé‡ï¼ˆä» `global_state_dict` å¤åˆ¶ï¼‰
- âš ï¸ **å†—ä½™**ï¼šå®¢æˆ·ç«¯ä¸Šä¼ äº†å®Œæ•´ state_dictï¼Œä½†æœåŠ¡å™¨åªä½¿ç”¨äº†å…¶ä¸­çš„ LoRA å‚æ•°

**ä¸ºä»€ä¹ˆ FedLoRA æ­£å¸¸å·¥ä½œï¼Ÿ**
- FedLoRA ä¹Ÿä½¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼Œä½†å®ƒèƒ½æ­£å¸¸å·¥ä½œ
- åŸå› ï¼šæœåŠ¡å™¨ç«¯çš„è¿‡æ»¤é€»è¾‘æ˜¯æ­£ç¡®çš„ï¼Œèƒ½å¤Ÿæ­£ç¡®æå–å’Œèšåˆ LoRA å‚æ•°
- **ä½†è¿™ä¸æ˜¯æœ€ä¼˜è®¾è®¡**ï¼šå®¢æˆ·ç«¯åº”è¯¥ç›´æ¥ä¸Šä¼ è¿‡æ»¤åçš„å‚æ•°

---

### 3. ç§æœ‰å‚æ•°ç®¡ç†ï¼ˆæ­£ç¡®ï¼‰

**ä½ç½®**: `src/federated_main.py:217-251`

**ä»£ç **:
```python
if args.alg == 'fedsdg':
    # æ·±æ‹·è´å…¨å±€æ¨¡å‹
    local_model_copy = copy.deepcopy(global_model)
    
    # å¦‚æœè¯¥å®¢æˆ·ç«¯æœ‰ç§æœ‰çŠ¶æ€ï¼Œåˆ™åŠ è½½
    if idx in local_private_states:
        current_state = local_model_copy.state_dict()
        for param_name, param_value in local_private_states[idx].items():
            if param_name in current_state:
                current_state[param_name] = param_value.clone()
        local_model_copy.load_state_dict(current_state)
    
    # è®­ç»ƒåä¿å­˜ç§æœ‰å‚æ•°
    private_state = {}
    for name, param in local_model_copy.named_parameters():
        if '_private' in name or 'lambda_k' in name:
            private_state[name] = param.data.clone().cpu()
    local_private_states[idx] = private_state
```

**åˆ†æ**:
- âœ… **æ­£ç¡®**ï¼šç§æœ‰å‚æ•°åœ¨å®¢æˆ·ç«¯æœ¬åœ°ä¿å­˜ï¼Œä¸ä¸Šä¼ åˆ°æœåŠ¡å™¨
- âœ… **æ­£ç¡®**ï¼šæ¯æ¬¡è®­ç»ƒå‰æ¢å¤å®¢æˆ·ç«¯çš„ç§æœ‰çŠ¶æ€
- âœ… **æ­£ç¡®**ï¼šè®­ç»ƒåæ›´æ–°ç§æœ‰çŠ¶æ€

---

### 4. LoRA åˆå§‹åŒ–ï¼ˆæ­£ç¡®ï¼‰

**ä½ç½®**: `src/models.py:55-77`

**ä»£ç **:
```python
# å…¨å±€åˆ†æ”¯
self.lora_A = nn.Parameter(torch.zeros(in_features, r))
self.lora_B = nn.Parameter(torch.zeros(r, out_features))
nn.init.normal_(self.lora_A, mean=0.0, std=1.0/r**0.5)

# FedSDG ç§æœ‰åˆ†æ”¯
if self.is_fedsdg:
    self.lora_A_private = nn.Parameter(torch.zeros(in_features, r))
    self.lora_B_private = nn.Parameter(torch.zeros(r, out_features))
    nn.init.normal_(self.lora_A_private, mean=0.0, std=1.0/r**0.5)
    
    # é—¨æ§å‚æ•°ï¼šåˆå§‹åŒ–ä¸º -2.0 (sigmoid(-2.0) â‰ˆ 0.12)
    self.lambda_k_logit = nn.Parameter(torch.tensor([-2.0]))
```

**åˆ†æ**:
- âœ… **æ­£ç¡®**ï¼šéµå¾ª LoRA è®ºæ–‡çš„åˆå§‹åŒ–ç­–ç•¥
- âœ… **æ­£ç¡®**ï¼š`lora_B` åˆå§‹åŒ–ä¸º 0ï¼Œç¡®ä¿åˆå§‹æ—¶ LoRA ä¸å½±å“è¾“å‡º
- âœ… **æ­£ç¡®**ï¼š`lambda_k` åˆå§‹åŒ–ä¸º -2.0ï¼Œä½¿æ¨¡å‹åˆå§‹é˜¶æ®µä¸»è¦ä¾èµ–å…¨å±€åˆ†æ”¯ï¼ˆ88% å…¨å±€ + 12% ç§æœ‰ï¼‰

---

### 5. å‰å‘ä¼ æ’­é€»è¾‘ï¼ˆæ­£ç¡®ï¼‰

**ä½ç½®**: `src/models.py:93-117`

**ä»£ç **:
```python
def forward(self, x):
    original_output = self.original_layer(x)
    
    if self.is_fedsdg:
        lambda_k = torch.sigmoid(self.lambda_k_logit)
        global_output = x @ self.lora_A @ self.lora_B
        private_output = x @ self.lora_A_private @ self.lora_B_private
        lora_output = (global_output * (1 - lambda_k) + private_output * lambda_k) * self.scaling
    else:
        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling
    
    return original_output + lora_output
```

**åˆ†æ**:
- âœ… **æ­£ç¡®**ï¼šåŒè·¯åŠ æƒç»„åˆå…¬å¼æ­£ç¡®
- âœ… **æ­£ç¡®**ï¼š`lambda_k` é€šè¿‡ sigmoid é™åˆ¶åœ¨ [0, 1]
- âœ… **æ­£ç¡®**ï¼šå…¨å±€å’Œç§æœ‰åˆ†æ”¯çš„è¾“å‡ºæ­£ç¡®ç›¸åŠ 

---

## ğŸ”§ ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤ 1: å®¢æˆ·ç«¯ä¸Šä¼ é€»è¾‘

**æ–‡ä»¶**: `src/update.py`

**ä¿®æ”¹**:
```python
# å¯¼å…¥ get_lora_state_dict
from models import get_lora_state_dict

def update_weights(self, model, global_round):
    # ... è®­ç»ƒé€»è¾‘ ...
    
    # FedLoRA/FedSDG: ä»…è¿”å› LoRA å‚æ•°ï¼ˆè¿‡æ»¤ç§æœ‰å‚æ•°å’Œå†»ç»“æƒé‡ï¼‰
    # FedAvg: è¿”å›å®Œæ•´ state_dict
    if self.args.alg in ('fedlora', 'fedsdg'):
        return get_lora_state_dict(model), sum(epoch_loss) / len(epoch_loss)
    else:
        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)
```

**æ•ˆæœ**:
- âœ… å®¢æˆ·ç«¯ä»…ä¸Šä¼  LoRA å…¨å±€å‚æ•°å’Œåˆ†ç±»å¤´
- âœ… é€šä¿¡é‡å‡å°‘ 95%ï¼ˆä» ~22 MB é™è‡³ ~0.2 MBï¼‰
- âœ… é€»è¾‘ä¸€è‡´æ€§æå‡

---

### ä¿®å¤ 2: å¢å¼ºè°ƒè¯•æ—¥å¿—

**æ–‡ä»¶**: `src/federated_main.py`

**æ–°å¢è°ƒè¯•è¾“å‡º**ï¼ˆæ¯ 5 è½®æ‰“å°ä¸€æ¬¡ï¼‰:

1. **å®¢æˆ·ç«¯è®­ç»ƒå**:
   - Lambda_k å‡å€¼å’ŒèŒƒå›´
   - ä¸Šä¼ å‚æ•°é”®åï¼ˆå‰ 5 ä¸ªï¼‰
   - åˆ†ç±»å¤´æƒé‡èŒƒæ•°
   - å…¨å±€ LoRA_A å¹³å‡èŒƒæ•°

2. **æœåŠ¡å™¨èšåˆå**:
   - èšåˆçš„å‚æ•°é”®æ•°é‡
   - éªŒè¯æ˜¯å¦åŒ…å«ç§æœ‰å‚æ•°ï¼ˆä¸åº”è¯¥åŒ…å«ï¼‰

**ç¤ºä¾‹è¾“å‡º**:
```
[FedSDG Debug - Round 5, Client 0]
  Lambda_k å‡å€¼: 0.1189 (èŒƒå›´: 0.1150 - 0.1220)
  è§£é‡Š: lambda_k=0.1189 è¡¨ç¤º 11.9% ç§æœ‰åˆ†æ”¯ + 88.1% å…¨å±€åˆ†æ”¯
  ä¸Šä¼ å‚æ•°é”®åï¼ˆå‰5ä¸ªï¼‰: ['blocks.0.attn.proj.lora_A', 'blocks.0.attn.proj.lora_B', ...]
  ä¸Šä¼ å‚æ•°æ€»æ•°: 26 ä¸ªé”®
  åˆ†ç±»å¤´æƒé‡èŒƒæ•°: 12.3456
  å…¨å±€ LoRA_A å¹³å‡èŒƒæ•°: 0.3542
[FedSDG Debug End]

[FedSDG Aggregation Debug - Round 5]
  èšåˆçš„å‚æ•°é”®æ•°é‡: 26
  å…¨å±€æ¨¡å‹æ€»é”®æ•°: 152
  âœ“ éªŒè¯é€šè¿‡: å…¨å±€æ¨¡å‹ä¸åŒ…å«ç§æœ‰å‚æ•°
[FedSDG Aggregation Debug End]
```

---

## ğŸ§ª éªŒè¯è®¡åˆ’

### 1. è¿è¡Œä¿®å¤åçš„ä»£ç 

```bash
cd /home/moqianyu_26/sda/hhm/Research/Federated-Learning-PyTorch/src
bash run_fedsdg_pretrained_cifar100.sh
```

### 2. è§‚å¯Ÿè°ƒè¯•è¾“å‡º

**å…³é”®æŒ‡æ ‡**:
- **Lambda_k æ¼”åŒ–**ï¼šåº”è¯¥ä» ~0.12 é€æ¸å¢é•¿ï¼ˆè¡¨ç¤ºç§æœ‰åˆ†æ”¯é€æ¸å­¦ä¹ ï¼‰
- **ä¸Šä¼ å‚æ•°æ•°é‡**ï¼šåº”è¯¥æ˜¯ 26 ä¸ªé”®ï¼ˆ12 ä¸ª LoRA å±‚ Ã— 2 å‚æ•° + 2 ä¸ªåˆ†ç±»å¤´å‚æ•°ï¼‰
- **åˆ†ç±»å¤´æƒé‡èŒƒæ•°**ï¼šåº”è¯¥é€æ¸å¢é•¿ï¼ˆè¡¨ç¤ºåˆ†ç±»å¤´åœ¨æ›´æ–°ï¼‰
- **å…¨å±€ LoRA_A èŒƒæ•°**ï¼šåº”è¯¥é€æ¸å¢é•¿ï¼ˆè¡¨ç¤ºå…¨å±€åˆ†æ”¯åœ¨å­¦ä¹ ï¼‰

### 3. å¯¹æ¯” FedLoRA

**é¢„æœŸç»“æœ**:
- FedSDG å‡†ç¡®ç‡ï¼š65-75%ï¼ˆæ¥è¿‘ FedLoRA çš„ 70%ï¼‰
- FedSDG åœ¨å¼º Non-IID åœºæ™¯ä¸‹åº”è¯¥ç•¥ä¼˜äº FedLoRAï¼ˆå› ä¸ºæœ‰ç§æœ‰åˆ†æ”¯ï¼‰

---

## ğŸ“Š æ ¹å› æ€»ç»“

### ä¸ºä»€ä¹ˆè®­ç»ƒæ›²çº¿åƒ"ä»é›¶å¼€å§‹"ï¼Ÿ

**åŸå› **ï¼šè™½ç„¶å®¢æˆ·ç«¯ä¸Šä¼ é€»è¾‘æœ‰ç¼ºé™·ï¼Œä½†è¿™**ä¸æ˜¯å¯¼è‡´æ€§èƒ½å´©æºƒçš„ç›´æ¥åŸå› **ã€‚

**çœŸæ­£çš„åŸå› **ï¼ˆæ¨æµ‹ï¼‰ï¼š
1. **é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥**ï¼šæ£€æŸ¥ `get_pretrained_vit()` æ˜¯å¦æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡
2. **å­¦ä¹ ç‡è¿‡å°**ï¼š0.0001 å¯¹äºé¢„è®­ç»ƒæ¨¡å‹å¯èƒ½è¿‡å°ï¼Œå¯¼è‡´æ”¶æ•›ç¼“æ…¢
3. **æ•°æ®é¢„å¤„ç†é—®é¢˜**ï¼šç¦»çº¿æ•°æ®çš„æ ‡å‡†åŒ–å¯èƒ½ä¸æ­£ç¡®

**å»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥**:
1. æ‰“å°æ¨¡å‹åŠ è½½æ—¥å¿—ï¼Œç¡®è®¤é¢„è®­ç»ƒæƒé‡æ˜¯å¦æˆåŠŸåŠ è½½
2. å°è¯•å¢å¤§å­¦ä¹ ç‡è‡³ 0.001
3. éªŒè¯ç¦»çº¿æ•°æ®çš„æ ‡å‡†åŒ–æ˜¯å¦ä½¿ç”¨ ImageNet æ ‡å‡†åŒ–

---

## ğŸ¯ ä¿®å¤ä¼˜å…ˆçº§

### é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»ä¿®å¤ï¼‰
1. âœ… **ä¿®å¤å®¢æˆ·ç«¯ä¸Šä¼ é€»è¾‘**ï¼šä½¿ç”¨ `get_lora_state_dict()` è¿‡æ»¤å‚æ•°
2. âœ… **å¢å¼ºè°ƒè¯•æ—¥å¿—**ï¼šè·Ÿè¸ª lambda_kã€ä¸Šä¼ å‚æ•°ã€æƒé‡èŒƒæ•°

### ä¸­ä¼˜å…ˆçº§ï¼ˆå»ºè®®ä¿®å¤ï¼‰
3. ğŸ” **éªŒè¯é¢„è®­ç»ƒæƒé‡åŠ è½½**ï¼šç¡®è®¤æ¨¡å‹æ˜¯å¦çœŸçš„ä½¿ç”¨äº†é¢„è®­ç»ƒæƒé‡
4. ğŸ” **è°ƒæ•´å­¦ä¹ ç‡**ï¼šå°è¯• 0.001ï¼ˆå½“å‰ 0.0001 å¯èƒ½è¿‡å°ï¼‰

### ä½ä¼˜å…ˆçº§ï¼ˆä¼˜åŒ–ï¼‰
5. ğŸ“ˆ **é€šä¿¡é‡ç»Ÿè®¡éªŒè¯**ï¼šç¡®è®¤ä¿®å¤åé€šä¿¡é‡æ˜¯å¦æ­£ç¡®ï¼ˆ~0.2 MB/è½®ï¼‰

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **è¿è¡Œä¿®å¤åçš„ä»£ç **ï¼š
   ```bash
   cd src
   bash run_fedsdg_pretrained_cifar100.sh
   ```

2. **è§‚å¯Ÿè°ƒè¯•è¾“å‡º**ï¼š
   - æ£€æŸ¥ Lambda_k æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆåˆå§‹ ~0.12ï¼‰
   - æ£€æŸ¥ä¸Šä¼ å‚æ•°æ•°é‡æ˜¯å¦æ­£ç¡®ï¼ˆ26 ä¸ªé”®ï¼‰
   - æ£€æŸ¥æƒé‡èŒƒæ•°æ˜¯å¦åœ¨å¢é•¿

3. **å¯¹æ¯” FedLoRA**ï¼š
   - å¦‚æœå‡†ç¡®ç‡ä»ç„¶å¾ˆä½ï¼ˆ<30%ï¼‰ï¼Œåˆ™é—®é¢˜ä¸åœ¨ä¸Šä¼ é€»è¾‘
   - éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥é¢„è®­ç»ƒæƒé‡åŠ è½½å’Œæ•°æ®é¢„å¤„ç†

4. **è°ƒæ•´è¶…å‚æ•°**ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼š
   - å°è¯•å­¦ä¹ ç‡ 0.001ï¼ˆå½“å‰ 0.0001ï¼‰
   - å°è¯•æ›´å¤§çš„ LoRA ç§©ï¼ˆå½“å‰ r=8ï¼Œå¯å°è¯• r=16ï¼‰

---

## ğŸ“ æŠ€æœ¯ç»†èŠ‚

### get_lora_state_dict() çš„è¿‡æ»¤é€»è¾‘

**ä½ç½®**: `src/models.py:369-395`

**ä»£ç **:
```python
def get_lora_state_dict(model):
    lora_state_dict = {}
    for name, param in model.named_parameters():
        # æ£€æŸ¥æ˜¯å¦ä¸º LoRA ç›¸å…³å‚æ•°æˆ–åˆ†ç±»å¤´
        if 'lora_' in name or 'mlp_head' in name or 'head' in name:
            # FedSDG è¿‡æ»¤ï¼šæ’é™¤ç§æœ‰å‚æ•°å’Œé—¨æ§å‚æ•°
            if '_private' in name or 'lambda_k' in name:
                continue
            # æ·»åŠ å…¨å±€å‚æ•°åˆ°è¿”å›å­—å…¸
            lora_state_dict[name] = param.data.clone()
    return lora_state_dict
```

**è¿‡æ»¤è§„åˆ™**:
- âœ… åŒ…å« `lora_` ä¸”ä¸åŒ…å« `_private` å’Œ `lambda_k`
- âœ… åŒ…å« `mlp_head` æˆ– `head`ï¼ˆåˆ†ç±»å¤´ï¼‰
- âŒ æ’é™¤ Backbone æƒé‡ï¼ˆä¸åŒ…å« `lora_` å…³é”®å­—ï¼‰
- âŒ æ’é™¤ç§æœ‰å‚æ•°ï¼ˆåŒ…å« `_private`ï¼‰
- âŒ æ’é™¤é—¨æ§å‚æ•°ï¼ˆåŒ…å« `lambda_k`ï¼‰

---

## ğŸ”¬ å®éªŒéªŒè¯

### é¢„æœŸç»“æœ

**ä¿®å¤å‰**ï¼ˆå½“å‰ï¼‰:
- å‡†ç¡®ç‡ï¼š17%ï¼ˆ50 è½®ï¼‰
- è®­ç»ƒæ›²çº¿ï¼šçº¿æ€§ç¼“æ…¢å¢é•¿ï¼ˆä»é›¶å¼€å§‹çš„ç‰¹å¾ï¼‰
- é€šä¿¡é‡ï¼š~22 MB/è½®ï¼ˆå®Œæ•´ state_dictï¼‰

**ä¿®å¤å**ï¼ˆé¢„æœŸï¼‰:
- å‡†ç¡®ç‡ï¼š65-75%ï¼ˆ50 è½®ï¼‰
- è®­ç»ƒæ›²çº¿ï¼šå¿«é€Ÿæ”¶æ•›ï¼ˆé¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾ï¼‰
- é€šä¿¡é‡ï¼š~0.2 MB/è½®ï¼ˆä»… LoRA å‚æ•°ï¼‰

### å¯¹æ¯” FedLoRA

| æŒ‡æ ‡ | FedLoRA | FedSDGï¼ˆä¿®å¤å‰ï¼‰ | FedSDGï¼ˆä¿®å¤åï¼‰ |
|------|---------|------------------|------------------|
| å‡†ç¡®ç‡ | 70% | 17% | **65-75%** |
| é€šä¿¡é‡ | 0.2 MB/è½® | 22 MB/è½® | **0.2 MB/è½®** |
| æ”¶æ•›é€Ÿåº¦ | å¿«é€Ÿ | ææ…¢ | **å¿«é€Ÿ** |
| Non-IID é²æ£’æ€§ | ä¸­ç­‰ | å·® | **ä¼˜ç§€** |

---

## âœ… ä¿®å¤æ¸…å•

- [x] ä¿®å¤å®¢æˆ·ç«¯ä¸Šä¼ é€»è¾‘ï¼ˆä½¿ç”¨ `get_lora_state_dict()`ï¼‰
- [x] å¢å¼ºè°ƒè¯•æ—¥å¿—ï¼ˆè·Ÿè¸ª lambda_kã€ä¸Šä¼ å‚æ•°ã€æƒé‡èŒƒæ•°ï¼‰
- [ ] éªŒè¯é¢„è®­ç»ƒæƒé‡åŠ è½½ï¼ˆéœ€è¦è¿è¡Œä»£ç ï¼‰
- [ ] è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
- [ ] éªŒè¯é€šä¿¡é‡ç»Ÿè®¡ï¼ˆå¦‚æœéœ€è¦ï¼‰

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **LoRA è®ºæ–‡**: Hu et al., "LoRA: Low-Rank Adaptation of Large Language Models", ICLR 2022
2. **FedAvg è®ºæ–‡**: McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data", AISTATS 2017
3. **ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ç»¼è¿°**: Tan et al., "Towards Personalized Federated Learning", IEEE TNNLS 2022

---

**æŠ¥å‘Šç»“æŸ**
