# 模型保存与分析系统使用指南

本文档介绍联邦学习训练过程中的模型保存模块，包括技术实现细节和使用方法。该系统支持 FedAvg、FedLoRA 和 FedSDG 三种算法，能够全面记录训练过程中的各类信息，方便后续分析和可视化。

## 目录

1. [功能概述](#功能概述)
2. [技术实现细节](#技术实现细节)
3. [保存的信息内容](#保存的信息内容)
4. [使用方法](#使用方法)
5. [数据加载与分析](#数据加载与分析)
6. [扩展性设计](#扩展性设计)

---

## 功能概述

### 解决的问题

该模型保存系统解决了以下关键问题：

| 问题 | 解决方案 |
|------|---------|
| 无法追踪每轮全局模型参数 | 按频率保存完整检查点 |
| 无法追踪 LoRA 参数的演化 | 记录 LoRA 参数范数历史 |
| 无法分析参数更新的幅度和方向 | 计算并保存参数 delta 和方向相似度 |
| 无法分析客户端之间的差异 | 保存每轮客户端本地权重 |
| 无法研究 Non-IID 数据对参数的影响 | 保存完整的客户端权重用于事后分析 |
| 聚合权重没有保存 | 返回并保存聚合权重和对齐度分数 |
| FedSDG 私有参数训练结束就丢失 | 持久化保存 `local_private_states` |
| 无法可视化门控系数变化 | 记录每轮门控参数统计信息 |

### 核心组件

1. **CheckpointManager**: 检查点管理器，负责保存和加载训练历史
2. **TrainingAnalyzer**: 训练分析器，提供各种分析功能
3. **analyze_training.py**: 独立的分析脚本，用于可视化和比较实验

---

## 技术实现细节

### 1. CheckpointManager 类

位置: `src/checkpoint_manager.py`

```python
class CheckpointManager:
    def __init__(
        self,
        save_dir: str,           # 保存目录
        experiment_name: str,     # 实验名称
        alg: str = 'fedavg',     # 算法类型
        save_frequency: int = 5,  # 保存频率
        save_client_weights: bool = True,  # 是否保存客户端权重
        ...
    )
```

#### 主要方法

- `save_round_checkpoint()`: 保存单轮检查点
- `save_final()`: 保存最终训练结果
- `load_checkpoint()`: 静态方法，加载检查点
- `load_training_history()`: 静态方法，加载训练历史
- `load_private_states()`: 静态方法，加载 FedSDG 私有状态

### 2. 聚合信息返回机制

修改了 `utils.py` 中的 `average_weights_lora()` 函数：

```python
def average_weights_lora(w, global_state_dict, agg_method='fedavg', 
                         epsilon=1e-8, return_aggregation_info=False):
    """
    新增参数:
        return_aggregation_info: 是否返回聚合信息
        
    返回:
        如果 return_aggregation_info=True:
            (global_weights, aggregation_info)
        否则:
            global_weights
    """
```

`aggregation_info` 字典包含：
- `weights`: 客户端聚合权重列表
- `alignment_scores`: 对齐度分数列表（仅 alignment 模式）
- `weight_stats`: 权重统计信息
- `agg_method`: 使用的聚合方法

### 3. 目录结构

```
save/checkpoints/
└── {experiment_name}_{timestamp}/
    ├── checkpoints/              # 每轮检查点
    │   ├── round_0000.pkl
    │   ├── round_0005.pkl
    │   └── ...
    ├── global_models/            # 全局模型（预留）
    ├── client_weights/           # 客户端权重（预留）
    ├── aggregation/              # 聚合信息（预留）
    ├── private_states/           # FedSDG 私有状态（预留）
    ├── param_evolution/          # 参数演化（预留）
    ├── training_history.pkl      # 训练历史
    ├── param_evolution.pkl       # 参数演化数据
    ├── final_model.pth           # 最终模型
    ├── final_private_states.pkl  # FedSDG 最终私有状态
    ├── experiment_summary.pkl    # 完整实验摘要
    └── summary.json              # 轻量级 JSON 摘要
```

---

## 保存的信息内容

### 1. 每轮检查点 (round_XXXX.pkl)

```python
{
    'round_idx': int,              # 轮次索引
    'timestamp': float,            # 时间戳
    'alg': str,                    # 算法类型
    'train_loss': float,           # 训练损失
    'train_acc': float,            # 训练准确率
    'test_acc': float,             # 测试准确率
    'test_loss': float,            # 测试损失
    'local_test_acc': float,       # 本地个性化准确率
    'local_test_loss': float,      # 本地个性化损失
    'comm_volume_mb': float,       # 累计通信量
    
    # 可选内容
    'global_state': dict,          # 全局模型 state_dict
    'local_weights': list,         # 客户端本地权重列表
    'local_losses': list,          # 客户端本地损失列表
    'selected_clients': list,      # 被选中的客户端 ID
    'aggregation_info': dict,      # 聚合信息
    'private_states': dict,        # FedSDG 私有状态
}
```

### 2. 训练历史 (training_history.pkl)

```python
{
    'rounds': list,                    # 轮次列表
    'train_loss': list,                # 训练损失历史
    'train_acc': list,                 # 训练准确率历史
    'test_acc': list,                  # 测试准确率历史
    'test_loss': list,                 # 测试损失历史
    'local_test_acc': list,            # 本地准确率历史
    'local_test_loss': list,           # 本地损失历史
    'comm_volume_mb': list,            # 通信量历史
    'timestamps': list,                # 时间戳历史
    
    # LoRA 参数演化
    'lora_param_norms': list,          # LoRA 参数范数历史
    
    # 聚合信息
    'aggregation_weights': list,       # 聚合权重历史
    'alignment_scores': list,          # 对齐度分数历史
    
    # FedSDG 门控参数
    'gate_values_mean': list,          # 门控均值历史
    'gate_values_std': list,           # 门控标准差历史
    'gate_values_min': list,           # 门控最小值历史
    'gate_values_max': list,           # 门控最大值历史
}
```

### 3. 参数演化 (param_evolution.pkl)

```python
{
    'rounds': list,                    # 轮次列表
    'param_deltas': list,              # 每轮参数更新量
    'update_magnitudes': list,         # 更新幅度历史
    'update_directions': list,         # 更新方向相似度历史
}
```

### 4. FedSDG 私有状态 (final_private_states.pkl)

```python
{
    client_id: {
        'transformer.layers.X.self_attn.out_proj.lora_A_private': tensor,
        'transformer.layers.X.self_attn.out_proj.lora_B_private': tensor,
        'transformer.layers.X.self_attn.out_proj.lambda_k_logit': tensor,
        ...
    },
    ...
}
```

---

## 使用方法

### 1. 启用检查点保存

检查点保存功能默认启用。可以通过命令行参数控制：

```bash
# 启用检查点保存（默认）
python federated_main.py --alg fedsdg --enable_checkpoint 1 ...

# 禁用检查点保存
python federated_main.py --alg fedavg --enable_checkpoint 0 ...

# 调整保存频率（每 10 轮保存一次详细检查点）
python federated_main.py --alg fedlora --save_frequency 10 ...

# 不保存客户端权重（节省磁盘空间）
python federated_main.py --alg fedsdg --save_client_weights 0 ...

# 限制最大检查点数量
python federated_main.py --alg fedsdg --max_checkpoints 20 ...
```

### 2. 新增的命令行参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `--enable_checkpoint` | int | 1 | 是否启用检查点保存 |
| `--save_frequency` | int | 5 | 详细检查点保存频率（每 N 轮） |
| `--save_client_weights` | int | 1 | 是否保存客户端本地权重 |
| `--max_checkpoints` | int | -1 | 最大检查点数量（-1 无限制） |

### 3. 示例训练命令

```bash
# FedSDG 训练（完整保存）
python federated_main.py \
    --alg fedsdg \
    --model vit \
    --model_variant pretrained \
    --dataset cifar \
    --epochs 50 \
    --num_users 100 \
    --frac 0.1 \
    --server_agg_method alignment \
    --enable_checkpoint 1 \
    --save_frequency 5 \
    --save_client_weights 1 \
    --gpu 0

# FedLoRA 训练（节省空间模式）
python federated_main.py \
    --alg fedlora \
    --model vit \
    --dataset cifar \
    --epochs 30 \
    --save_frequency 10 \
    --save_client_weights 0 \
    --gpu 0
```

---

## 数据加载与分析

### 1. 使用 Python 代码加载数据

```python
from checkpoint_manager import CheckpointManager, TrainingAnalyzer

# 加载训练历史
history = CheckpointManager.load_training_history('/path/to/experiment')

# 查看测试准确率演化
import matplotlib.pyplot as plt
plt.plot(history['rounds'], history['test_acc'])
plt.xlabel('Round')
plt.ylabel('Test Accuracy')
plt.show()

# 加载特定检查点
checkpoint = CheckpointManager.load_checkpoint('/path/to/checkpoint/round_0050.pkl')
print(f"Round {checkpoint['round_idx']}: Test Acc = {checkpoint['test_acc']}")

# 使用分析器
analyzer = TrainingAnalyzer('/path/to/experiment')

# 获取 LoRA 参数演化
lora_evolution = analyzer.get_lora_param_evolution()

# 获取聚合权重演化
agg_weights = analyzer.get_aggregation_weights_evolution()

# 获取门控参数演化（FedSDG）
gate_evolution = analyzer.get_gate_values_evolution()

# 导出可视化数据
analyzer.export_for_visualization('visualization_data.json')
```

### 2. 使用分析脚本

```bash
# 基本分析
python analyze_training.py --experiment_dir /path/to/experiment

# 生成可视化图表
python analyze_training.py --experiment_dir /path/to/experiment --visualize

# 导出 JSON 数据
python analyze_training.py --experiment_dir /path/to/experiment --export_json

# 分析特定检查点
python analyze_training.py --experiment_dir /path/to/experiment --checkpoint_round 50

# 比较两个实验
python analyze_training.py \
    --experiment_dir /path/to/experiment1 \
    --compare /path/to/experiment2 \
    --visualize
```

### 3. 生成的可视化图表

运行 `--visualize` 后会在 `analysis/` 目录下生成：

- **training_curves.png**: 训练曲线（准确率、损失、通信量）
- **param_evolution.png**: 参数演化图（更新幅度、方向一致性）
- **aggregation_analysis.png**: 聚合权重分析（权重方差、分布、热力图）
- **gate_evolution.png**: 门控参数演化（FedSDG 专用）
- **comparison.png**: 实验比较图（如使用 `--compare`）

### 4. 分析客户端差异

```python
from checkpoint_manager import TrainingAnalyzer

analyzer = TrainingAnalyzer('/path/to/experiment')

# 分析特定轮次的客户端差异
diversity = analyzer.analyze_client_diversity('/path/to/checkpoint/round_0050.pkl')
print(f"客户端间平均差异: {diversity['mean_distance']:.6f}")
print(f"最大差异: {diversity['max_distance']:.6f}")
print(f"最小差异: {diversity['min_distance']:.6f}")
```

### 5. 加载 FedSDG 私有状态进行分析

```python
from checkpoint_manager import CheckpointManager
import torch

# 加载最终私有状态
private_states = CheckpointManager.load_private_states('/path/to/experiment')

# 分析门控参数
for client_id, state in private_states.items():
    gate_values = []
    for name, param in state.items():
        if 'lambda_k_logit' in name:
            m_k = torch.sigmoid(param).item()
            gate_values.append(m_k)
    
    print(f"Client {client_id}: 门控均值 = {np.mean(gate_values):.4f}")
```

---

## 扩展性设计

### 1. 添加新的联邦学习算法

要支持新算法（如 FedProx），只需：

1. 在 `CheckpointManager.__init__()` 中添加算法识别
2. 在 `save_round_checkpoint()` 中添加算法特定的保存逻辑
3. 在 `TrainingAnalyzer` 中添加分析方法

示例：

```python
class CheckpointManager:
    def __init__(self, ..., alg='fedavg'):
        # 添加 FedProx 支持
        if alg == 'fedprox':
            self.training_history['proximal_term'] = []
    
    def save_round_checkpoint(self, ..., proximal_term=None):
        # 保存 FedProx 特定信息
        if self.alg == 'fedprox' and proximal_term is not None:
            self.training_history['proximal_term'].append(proximal_term)
```

### 2. 添加新的分析指标

在 `TrainingAnalyzer` 中添加新方法：

```python
class TrainingAnalyzer:
    def get_custom_metric(self) -> Dict:
        """自定义指标计算"""
        # 实现自定义分析逻辑
        pass
```

### 3. 自定义可视化

在 `analyze_training.py` 中添加新的可视化函数：

```python
def visualize_custom_metric(data: Dict, output_dir: str):
    """自定义可视化"""
    # 实现可视化逻辑
    pass
```

---

## 最佳实践

### 1. 磁盘空间管理

- 对于长时间训练，建议设置 `--max_checkpoints` 限制检查点数量
- 如果磁盘空间有限，可以禁用客户端权重保存 `--save_client_weights 0`
- 增大 `--save_frequency` 可以减少检查点数量

### 2. 性能优化

- 检查点保存在训练循环中执行，会略微增加每轮时间
- 保存客户端权重会增加内存和磁盘使用
- 对于大规模实验，建议适当增大保存频率

### 3. 实验管理

- 每次实验会自动生成带时间戳的目录名，避免覆盖
- 使用 `summary.json` 快速查看实验结果
- 使用 `analyze_training.py --compare` 比较不同算法的效果

---

## 常见问题

### Q1: 如何找到保存的检查点？

检查点保存在 `save/checkpoints/{experiment_name}_{timestamp}/` 目录下。

### Q2: 如何减少磁盘占用？

```bash
# 减少保存频率
--save_frequency 20

# 不保存客户端权重
--save_client_weights 0

# 限制检查点数量
--max_checkpoints 10
```

### Q3: 如何禁用检查点保存？

```bash
--enable_checkpoint 0
```

### Q4: 分析脚本报错怎么办？

确保实验目录中包含 `training_history.pkl` 文件。如果实验提前终止，可能只有部分数据。

### Q5: 如何只保存最终模型？

```bash
--enable_checkpoint 0  # 禁用检查点管理器
```

最终模型仍会保存在 `save/models/` 目录下。

---

## 版本历史

- **v1.0** (2024-01): 初始版本
  - 支持 FedAvg、FedLoRA、FedSDG 三种算法
  - 完整的检查点保存和加载功能
  - 训练历史分析工具
  - 可视化脚本

---

## 联系方式

如有问题或建议，请联系项目维护者。

