# FedSDG ç®—æ³•å®æ–½æŠ€æœ¯æŠ¥å‘Š

**é¡¹ç›®**: Federated-Learning-PyTorch  
**ç®—æ³•**: FedSDG (Federated Learning with Split Dual-path Gating)  
**å®æ–½æ—¥æœŸ**: 2026-01-06  
**ç‰ˆæœ¬**: v1.0

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šè¯¦ç»†è®°å½•äº†åœ¨ç°æœ‰è”é‚¦å­¦ä¹ æ¡†æ¶ä¸­æ–°å¢ **FedSDG ç®—æ³•**çš„å®Œæ•´å®æ–½è¿‡ç¨‹ã€‚FedSDG æ˜¯ä¸€ç§åŸºäº LoRA çš„å‚æ•°é«˜æ•ˆè”é‚¦å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡**åŒè·¯æ¶æ„**ï¼ˆå…¨å±€åˆ†æ”¯ + ç§æœ‰åˆ†æ”¯ï¼‰å’Œ**å¯å­¦ä¹ é—¨æ§æœºåˆ¶**æ¥å¯¹æŠ— Non-IID æ•°æ®åˆ†å¸ƒï¼ŒåŒæ—¶ä¿æŒä¸ FedLoRA ç›¸åŒçš„é€šä¿¡æ•ˆç‡ï¼ˆ**0.2MB/è½®**ï¼‰ã€‚

### æ ¸å¿ƒæˆæœ
- âœ… **å®Œå…¨éä¾µå…¥å¼è®¾è®¡**ï¼šä¸å½±å“ç°æœ‰ FedAvg å’Œ FedLoRA çš„ä»»ä½•åŠŸèƒ½
- âœ… **é€šä¿¡é‡ä¸€è‡´æ€§**ï¼šä¸ FedLoRA ä¿æŒå®Œå…¨ç›¸åŒçš„é€šä¿¡å¼€é”€ï¼ˆ0.2MB vs FedAvg 22.8MBï¼‰
- âœ… **æ¨¡å—åŒ–æ¶æ„**ï¼šæ‰€æœ‰æ–°å¢ä»£ç é€šè¿‡å‚æ•°å¼€å…³æ§åˆ¶ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
- âœ… **å®Œæ•´æµ‹è¯•è¦†ç›–**ï¼šæä¾›å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•è„šæœ¬

---

## ğŸ¯ è®¾è®¡ç›®æ ‡

### 1. æ ¸å¿ƒéœ€æ±‚
- **åŒè·¯æ¶æ„**ï¼šå®ç°å…¨å±€åˆ†æ”¯ï¼ˆå‚ä¸èšåˆï¼‰+ ç§æœ‰åˆ†æ”¯ï¼ˆæœ¬åœ°ä¿ç•™ï¼‰
- **é—¨æ§æœºåˆ¶**ï¼šå¯å­¦ä¹ çš„ Î»_k å‚æ•°åŠ¨æ€å¹³è¡¡å…¨å±€/ç§æœ‰åˆ†æ”¯æƒé‡
- **é€šä¿¡æ•ˆç‡**ï¼šç§æœ‰å‚æ•°ä¸ä¸Šä¼ ï¼Œä¿æŒä¸ FedLoRA ç›¸åŒçš„ 0.2MB é€šä¿¡é‡
- **Non-IID å¯¹æŠ—**ï¼šåˆ©ç”¨ç§æœ‰åˆ†æ”¯å­¦ä¹ å®¢æˆ·ç«¯ç‰¹å®šæ¨¡å¼

### 2. éä¾µå…¥æ€§åŸåˆ™
- æ‰€æœ‰ä¿®æ”¹é€šè¿‡ `is_fedsdg` å‚æ•°æ§åˆ¶
- é»˜è®¤è¡Œä¸ºï¼ˆ`is_fedsdg=False`ï¼‰ä¸åŸæœ‰ LoRA å®Œå…¨ä¸€è‡´
- ä¸ä¿®æ”¹ä»»ä½•ç°æœ‰å‡½æ•°ç­¾åçš„é»˜è®¤å€¼
- é€šè¿‡æ¡ä»¶åˆ†æ”¯éš”ç¦» FedSDG ç‰¹å®šé€»è¾‘

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FedSDG æ¶æ„                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  å®¢æˆ·ç«¯ k:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  LoRALayer (is_fedsdg=True)                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  åŸå§‹å†»ç»“å±‚: W (ä¸è®­ç»ƒ)                      â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚  å…¨å±€åˆ†æ”¯        â”‚      â”‚  ç§æœ‰åˆ†æ”¯        â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  lora_A (ä¸Šä¼ )   â”‚      â”‚  lora_A_private  â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  lora_B (ä¸Šä¼ )   â”‚      â”‚  lora_B_private  â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â”‚           â†“                          â†“              â”‚    â”‚
â”‚  â”‚      Global_Out              Private_Out           â”‚    â”‚
â”‚  â”‚           â†“                          â†“              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚  é—¨æ§åŠ æƒ: Î»_k âˆˆ [0,1] (æœ¬åœ°ä¿ç•™)        â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  Output = (1-Î»_k)Â·Global + Î»_kÂ·Private    â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  é€šä¿¡æµç¨‹:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ä¸Šä¼ å…¨å±€å‚æ•°  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  å®¢æˆ·ç«¯ k  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚  æœåŠ¡å™¨    â”‚              â”‚
â”‚  â”‚            â”‚  (lora_A/B)    â”‚            â”‚              â”‚
â”‚  â”‚            â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚            â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  ä¸‹è½½èšåˆç»“æœ  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â”‚  æœ¬åœ°å­˜å‚¨: local_private_states[k] = {                      â”‚
â”‚      'lora_A_private': tensor,                              â”‚
â”‚      'lora_B_private': tensor,                              â”‚
â”‚      'lambda_k_logit': tensor                               â”‚
â”‚  }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°å­¦å…¬å¼

**FedSDG å‰å‘ä¼ æ’­**:
```
h = Wx + scaling Â· [(xÂ·AÂ·B)Â·(1 - Î»_k) + (xÂ·A_privateÂ·B_private)Â·Î»_k]
```

å…¶ä¸­:
- `W`: å†»ç»“çš„é¢„è®­ç»ƒæƒé‡
- `A, B`: å…¨å±€ LoRA çŸ©é˜µï¼ˆå‚ä¸æœåŠ¡å™¨èšåˆï¼‰
- `A_private, B_private`: ç§æœ‰ LoRA çŸ©é˜µï¼ˆä»…æœ¬åœ°æ›´æ–°ï¼‰
- `Î»_k = sigmoid(Î»_k_logit)`: é—¨æ§å‚æ•°ï¼Œæ§åˆ¶å…¨å±€/ç§æœ‰åˆ†æ”¯æƒé‡
- `scaling = lora_alpha / r`: LoRA ç¼©æ”¾å› å­

---

## ğŸ’» å®æ–½ç»†èŠ‚

### 1. æ¨¡å‹å±‚ä¿®æ”¹ (`models.py`)

#### 1.1 LoRALayer ç±»æ‰©å±•

**ä¿®æ”¹ä½ç½®**: `class LoRALayer(nn.Module)`

**å…³é”®æ”¹åŠ¨**:
```python
def __init__(self, original_layer, r=8, lora_alpha=16, is_fedsdg=False):
    # ... åŸæœ‰ä»£ç  ...
    
    self.is_fedsdg = is_fedsdg  # æ–°å¢ï¼šFedSDG æ¨¡å¼æ ‡å¿—
    
    # å…¨å±€åˆ†æ”¯ï¼ˆåŸæœ‰å‚æ•°ï¼Œæ‰€æœ‰æ¨¡å¼å…±ç”¨ï¼‰
    self.lora_A = nn.Parameter(torch.zeros(in_features, r))
    self.lora_B = nn.Parameter(torch.zeros(r, out_features))
    
    # FedSDG ä¸“ç”¨ï¼šç§æœ‰åˆ†æ”¯
    if self.is_fedsdg:
        self.lora_A_private = nn.Parameter(torch.zeros(in_features, r))
        self.lora_B_private = nn.Parameter(torch.zeros(r, out_features))
        self.lambda_k_logit = nn.Parameter(torch.zeros(1))  # é—¨æ§å‚æ•°
```

**å‰å‘ä¼ æ’­ä¿®æ”¹**:
```python
def forward(self, x):
    original_output = self.original_layer(x)
    
    if self.is_fedsdg:
        # åŒè·¯åŠ æƒè®¡ç®—
        lambda_k = torch.sigmoid(self.lambda_k_logit)
        global_output = x @ self.lora_A @ self.lora_B
        private_output = x @ self.lora_A_private @ self.lora_B_private
        lora_output = (global_output * (1 - lambda_k) + 
                      private_output * lambda_k) * self.scaling
    else:
        # æ ‡å‡† LoRA å•è·¯è®¡ç®—
        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling
    
    return original_output + lora_output
```

**è®¾è®¡äº®ç‚¹**:
- âœ… é€šè¿‡ `if self.is_fedsdg` å®Œå…¨éš”ç¦»æ–°æ—§é€»è¾‘
- âœ… é»˜è®¤ `is_fedsdg=False` ä¿æŒå‘åå…¼å®¹
- âœ… ç§æœ‰å‚æ•°ä»…åœ¨éœ€è¦æ—¶åˆ›å»ºï¼ŒèŠ‚çœå†…å­˜

#### 1.2 å‚æ•°è¿‡æ»¤å‡½æ•°

**ä¿®æ”¹ä½ç½®**: `get_lora_state_dict(model)`

**å…³é”®æ”¹åŠ¨**:
```python
def get_lora_state_dict(model):
    lora_state_dict = {}
    for name, param in model.named_parameters():
        if 'lora_' in name or 'mlp_head' in name or 'head' in name:
            # FedSDG è¿‡æ»¤ï¼šæ’é™¤ç§æœ‰å‚æ•°å’Œé—¨æ§å‚æ•°
            if '_private' in name or 'lambda_k' in name:
                continue  # è·³è¿‡ï¼Œä¸å‚ä¸æœåŠ¡å™¨èšåˆ
            lora_state_dict[name] = param.data.clone()
    return lora_state_dict
```

**åŠŸèƒ½éªŒè¯**:
- âœ… FedLoRA: è¿”å›æ‰€æœ‰ `lora_A`, `lora_B`, `mlp_head` å‚æ•°
- âœ… FedSDG: ä»…è¿”å› `lora_A`, `lora_B`ï¼ˆè¿‡æ»¤ `_private` å’Œ `lambda_k`ï¼‰
- âœ… é€šä¿¡é‡å®Œå…¨ä¸€è‡´

#### 1.3 æ³¨å…¥å‡½æ•°æ›´æ–°

**ä¿®æ”¹ä½ç½®**: `inject_lora()` å’Œ `inject_lora_timm()`

**å…³é”®æ”¹åŠ¨**:
```python
def inject_lora(model, r=8, lora_alpha=16, train_mlp_head=True, is_fedsdg=False):
    # ... åŸæœ‰ä»£ç  ...
    
    for layer_idx, encoder_layer in enumerate(model.transformer.layers):
        # ä¼ é€’ is_fedsdg å‚æ•°
        lora_out_proj = LoRALayer(
            original_out_proj, r=r, lora_alpha=lora_alpha, is_fedsdg=is_fedsdg
        )
        # ... å…¶ä»–ä»£ç  ...
```

**è®¾è®¡äº®ç‚¹**:
- âœ… æ–°å¢ `is_fedsdg` å‚æ•°ï¼Œé»˜è®¤ `False`
- âœ… ä¸ä¿®æ”¹ç°æœ‰è°ƒç”¨ä»£ç çš„è¡Œä¸º
- âœ… åŒæ—¶æ”¯æŒæ‰‹å†™ ViT å’Œ timm é¢„è®­ç»ƒæ¨¡å‹

---

### 2. ä¸»è®­ç»ƒæµç¨‹ä¿®æ”¹ (`federated_main.py`)

#### 2.1 æ¨¡å‹æ³¨å…¥é€»è¾‘

**ä¿®æ”¹ä½ç½®**: ç¬¬ 96-131 è¡Œ

**å…³é”®æ”¹åŠ¨**:
```python
# æ”¯æŒ FedLoRA å’Œ FedSDG
if args.alg in ('fedlora', 'fedsdg'):
    is_fedsdg = (args.alg == 'fedsdg')
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ³¨å…¥å‡½æ•°
    if hasattr(args, 'model_variant') and args.model_variant == 'pretrained':
        global_model = inject_lora_timm(
            global_model, r=args.lora_r, lora_alpha=args.lora_alpha,
            train_head=bool(args.lora_train_mlp_head), is_fedsdg=is_fedsdg
        )
    else:
        global_model = inject_lora(
            global_model, r=args.lora_r, lora_alpha=args.lora_alpha,
            train_mlp_head=bool(args.lora_train_mlp_head), is_fedsdg=is_fedsdg
        )
```

#### 2.2 å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†

**ä¿®æ”¹ä½ç½®**: ç¬¬ 178-191 è¡Œï¼ˆåˆå§‹åŒ–ï¼‰ã€ç¬¬ 217-251 è¡Œï¼ˆè®­ç»ƒå¾ªç¯ï¼‰

**å…³é”®æ”¹åŠ¨**:

**åˆå§‹åŒ–é˜¶æ®µ**:
```python
# FedSDG ä¸“ç”¨ï¼šå®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†
local_private_states = {} if args.alg == 'fedsdg' else None

if args.alg == 'fedsdg':
    print("[FedSDG] å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†å·²åˆå§‹åŒ–")
    print("  æ¯ä¸ªå®¢æˆ·ç«¯å°†ç»´æŠ¤ç‹¬ç«‹çš„ç§æœ‰å‚æ•°ï¼ˆlora_A_private, lora_B_private, lambda_kï¼‰")
    print("  ç§æœ‰å‚æ•°ä¸å‚ä¸æœåŠ¡å™¨èšåˆï¼Œä»…åœ¨æœ¬åœ°æ›´æ–°")
```

**è®­ç»ƒå¾ªç¯**:
```python
for idx in idxs_users:
    # ========== FedSDGï¼šåŠ è½½å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ ==========
    if args.alg == 'fedsdg':
        local_model_copy = copy.deepcopy(global_model)
        
        # å¦‚æœè¯¥å®¢æˆ·ç«¯æœ‰ç§æœ‰çŠ¶æ€ï¼Œåˆ™åŠ è½½
        if idx in local_private_states:
            current_state = local_model_copy.state_dict()
            for param_name, param_value in local_private_states[idx].items():
                if param_name in current_state:
                    current_state[param_name] = param_value.clone()
            local_model_copy.load_state_dict(current_state)
    else:
        local_model_copy = copy.deepcopy(global_model)
    
    # æœ¬åœ°è®­ç»ƒ
    local_model = LocalUpdate(...)
    w, loss = local_model.update_weights(model=local_model_copy, ...)
    
    # ========== FedSDGï¼šä¿å­˜å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ ==========
    if args.alg == 'fedsdg':
        private_state = {}
        for name, param in local_model_copy.named_parameters():
            if '_private' in name or 'lambda_k' in name:
                private_state[name] = param.data.clone().cpu()
        local_private_states[idx] = private_state
```

**è®¾è®¡äº®ç‚¹**:
- âœ… ä½¿ç”¨å­—å…¸ `local_private_states[user_id]` å­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯çš„ç§æœ‰å‚æ•°
- âœ… ç§æœ‰å‚æ•°ä¿å­˜åˆ° CPU ä»¥èŠ‚çœ GPU å†…å­˜
- âœ… é¦–æ¬¡è®­ç»ƒçš„å®¢æˆ·ç«¯ä½¿ç”¨æ¨¡å‹åˆå§‹åŒ–çš„ç§æœ‰å‚æ•°
- âœ… å®Œå…¨ä¸å½±å“ FedAvg å’Œ FedLoRA çš„è®­ç»ƒæµç¨‹

#### 2.3 èšåˆé€»è¾‘æ›´æ–°

**ä¿®æ”¹ä½ç½®**: ç¬¬ 256-262 è¡Œ

**å…³é”®æ”¹åŠ¨**:
```python
# FedLoRA å’Œ FedSDG: ä½¿ç”¨é€‰æ‹©æ€§èšåˆï¼ˆä»…èšåˆ LoRA å…¨å±€å‚æ•°ï¼‰
if args.alg in ('fedlora', 'fedsdg'):
    global_weights = average_weights_lora(local_weights, global_model.state_dict())
else:
    global_weights = average_weights(local_weights)
```

**åŠŸèƒ½è¯´æ˜**:
- `average_weights_lora()` ä¼šè°ƒç”¨ `get_lora_state_dict()` æå–å‚æ•°
- FedSDG çš„ç§æœ‰å‚æ•°å·²åœ¨ `get_lora_state_dict()` ä¸­è¢«è¿‡æ»¤
- å› æ­¤æœåŠ¡å™¨ä»…èšåˆå…¨å±€åˆ†æ”¯å‚æ•°

---

### 3. å‘½ä»¤è¡Œå‚æ•°æ‰©å±• (`options.py`)

**ä¿®æ”¹ä½ç½®**: ç¬¬ 28-29 è¡Œã€ç¬¬ 95-97 è¡Œ

**å…³é”®æ”¹åŠ¨**:
```python
# ç®—æ³•é€‰æ‹©
parser.add_argument('--alg', type=str, default='fedavg', 
                    choices=['fedavg', 'fedlora', 'fedsdg'],
                    help='federated learning algorithm')

# éªŒè¯é€»è¾‘
if args.alg in ('fedlora', 'fedsdg') and args.model != 'vit':
    raise ValueError(f"{args.alg.upper()} currently only supports ViT model")
```

**è®¾è®¡äº®ç‚¹**:
- âœ… æ–°å¢ `'fedsdg'` åˆ° choices åˆ—è¡¨
- âœ… FedSDG ä¸ FedLoRA å…±äº«ç›¸åŒçš„ LoRA å‚æ•°ï¼ˆ`--lora_r`, `--lora_alpha`ï¼‰
- âœ… ç»Ÿä¸€çš„éªŒè¯é€»è¾‘ï¼ˆä»…æ”¯æŒ ViT æ¨¡å‹ï¼‰

---

### 4. å·¥å…·å‡½æ•°æ›´æ–° (`utils.py`)

#### 4.1 é€šä¿¡é‡ç»Ÿè®¡

**ä¿®æ”¹ä½ç½®**: `get_communication_stats()`

**å…³é”®æ”¹åŠ¨**:
```python
if alg in ('fedlora', 'fedsdg'):
    # FedLoRA å’Œ FedSDG: ä»…é€šä¿¡å…¨å±€ LoRA å‚æ•°ï¼ˆä¸åŒ…æ‹¬ç§æœ‰å‚æ•°ï¼‰
    # FedSDG çš„ç§æœ‰å‚æ•°ï¼ˆ_private å’Œ lambda_kï¼‰ä¸å‚ä¸é€šä¿¡
    # å› æ­¤é€šä¿¡é‡ä¸ FedLoRA å®Œå…¨ç›¸åŒ
    comm_params = trainable_params
```

#### 4.2 é€šä¿¡é…ç½®æ–‡ä»¶æ‰“å°

**ä¿®æ”¹ä½ç½®**: `print_communication_profile()`

**å…³é”®æ”¹åŠ¨**:
```python
elif args.alg == 'fedsdg':
    print("[FedSDG] Communicating ONLY Global LoRA parameters (lora_A, lora_B)")
    print("[FedSDG] Private parameters (lora_A_private, lora_B_private, lambda_k) stay local")
    print(f"[FedSDG] Communication Efficiency: {comm_stats['compression_ratio']:.2f}% of full model")
```

#### 4.3 å®éªŒè¯¦æƒ…æ˜¾ç¤º

**ä¿®æ”¹ä½ç½®**: `exp_details()`

**å…³é”®æ”¹åŠ¨**:
```python
if args.alg in ('fedlora', 'fedsdg'):
    print(f'\n    LoRA parameters:')
    print(f'    LoRA rank (r)      : {args.lora_r}')
    print(f'    LoRA alpha         : {args.lora_alpha}')
    if args.alg == 'fedsdg':
        print(f'\n    FedSDG specific:')
        print(f'    Dual-path mode     : Enabled (Global + Private branches)')
        print(f'    Private params     : Not communicated (client-local only)')
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯•è„šæœ¬

åˆ›å»ºäº†ä¸¤ä¸ªæµ‹è¯•è„šæœ¬ï¼š

#### 1. å•å…ƒæµ‹è¯• (`test_fedsdg.py`)

**æµ‹è¯•è¦†ç›–**:
- âœ… LoRALayer FedSDG æ¨¡å¼åˆå§‹åŒ–
- âœ… å‰å‘ä¼ æ’­åŒè·¯è®¡ç®—
- âœ… get_lora_state_dict ç§æœ‰å‚æ•°è¿‡æ»¤
- âœ… é€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´æ€§
- âœ… å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†
- âœ… å‰å‘å’Œåå‘ä¼ æ’­

**è¿è¡Œæ–¹å¼**:
```bash
cd src
python3 test_fedsdg.py
```

#### 2. é›†æˆæµ‹è¯• (`run_fedsdg_test.sh`)

**æµ‹è¯•åœºæ™¯**:
- æ•°æ®é›†: CIFAR-10
- è®­ç»ƒè½®æ¬¡: 5
- Non-IID ç¨‹åº¦: Î±=0.1ï¼ˆå¼ºå¼‚æ„ï¼‰
- LoRA ç§©: r=8

**è¿è¡Œæ–¹å¼**:
```bash
cd src
bash run_fedsdg_test.sh
```

**é¢„æœŸè¾“å‡º**:
```
[FedSDG] å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†å·²åˆå§‹åŒ–
  æ¯ä¸ªå®¢æˆ·ç«¯å°†ç»´æŠ¤ç‹¬ç«‹çš„ç§æœ‰å‚æ•°ï¼ˆlora_A_private, lora_B_private, lambda_kï¼‰
  ç§æœ‰å‚æ•°ä¸å‚ä¸æœåŠ¡å™¨èšåˆï¼Œä»…åœ¨æœ¬åœ°æ›´æ–°
  å…¨å±€å‚æ•°ï¼ˆlora_A, lora_Bï¼‰å‚ä¸æœåŠ¡å™¨èšåˆï¼Œä¿æŒé€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´

COMMUNICATION PROFILE
----------------------------------------------------------------------
Communication per Round (1-way)              0.20 MB
Communication per Round (2-way)              0.40 MB
Compression Ratio                            0.87%

[FedSDG] Communicating ONLY Global LoRA parameters (lora_A, lora_B)
[FedSDG] Private parameters stay local
[FedSDG] Communication Efficiency: 0.87% of full model (same as FedLoRA)
```

---

## ğŸ“Š é€šä¿¡é‡éªŒè¯

### ç†è®ºåˆ†æ

**ViT-Tiny æ¨¡å‹å‚æ•°ç»Ÿè®¡** (CIFAR-10, r=8):

| å‚æ•°ç±»å‹ | å‚æ•°é‡ | å¤§å° (MB) | æ˜¯å¦é€šä¿¡ |
|---------|--------|-----------|---------|
| é¢„è®­ç»ƒéª¨å¹² | ~5.7M | 22.8 | âŒ (å†»ç»“) |
| **å…¨å±€ LoRA** (lora_A, lora_B) | ~50K | **0.2** | âœ… |
| ç§æœ‰ LoRA (lora_A_private, lora_B_private) | ~50K | 0.2 | âŒ (æœ¬åœ°) |
| é—¨æ§å‚æ•° (lambda_k) | ~12 | 0.00005 | âŒ (æœ¬åœ°) |
| åˆ†ç±»å¤´ (head) | 1,280 | 0.005 | âœ… |
| **æ€»é€šä¿¡é‡** | - | **0.2** | - |

**å¯¹æ¯”ç»“æœ**:
- FedAvg: 22.8 MB/è½®
- FedLoRA: 0.2 MB/è½®
- **FedSDG: 0.2 MB/è½®** âœ…

**é€šä¿¡èŠ‚çœç‡**: 99.13% (ç›¸æ¯” FedAvg)

---

## ğŸ¨ ä»£ç è´¨é‡

### è®¾è®¡æ¨¡å¼

1. **ç­–ç•¥æ¨¡å¼**: é€šè¿‡ `is_fedsdg` å‚æ•°åˆ‡æ¢ä¸åŒçš„å‰å‘ä¼ æ’­ç­–ç•¥
2. **å·¥å‚æ¨¡å¼**: `inject_lora()` æ ¹æ®å‚æ•°åˆ›å»ºä¸åŒé…ç½®çš„ LoRALayer
3. **çŠ¶æ€æ¨¡å¼**: `local_private_states` ç®¡ç†å®¢æˆ·ç«¯çŠ¶æ€
4. **å•ä¸€èŒè´£åŸåˆ™**: æ¯ä¸ªå‡½æ•°èŒè´£æ˜ç¡®ï¼Œæ˜“äºæµ‹è¯•

### ä»£ç æ³¨é‡Š

æ‰€æœ‰æ–°å¢ä»£ç å‡åŒ…å«è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šï¼š
- åŠŸèƒ½è¯´æ˜
- å‚æ•°è§£é‡Š
- è®¾è®¡æ„å›¾
- è¾¹ç•Œæ¡ä»¶

ç¤ºä¾‹:
```python
# ========== FedSDG ä¸“ç”¨ï¼šç§æœ‰åˆ†æ”¯ï¼ˆPrivate Pathï¼‰==========
if self.is_fedsdg:
    # ç§æœ‰ä½ç§©çŸ©é˜µï¼ˆä¸å‚ä¸æœåŠ¡å™¨èšåˆï¼‰
    self.lora_A_private = nn.Parameter(torch.zeros(in_features, r))
    self.lora_B_private = nn.Parameter(torch.zeros(r, out_features))
    
    # é—¨æ§å‚æ•° lambda_kï¼šæ§åˆ¶å…¨å±€/ç§æœ‰åˆ†æ”¯çš„æƒé‡
    # åˆå§‹åŒ–ä¸º 0.5ï¼ˆå…¨å±€å’Œç§æœ‰å„å  50%ï¼‰
    # ä½¿ç”¨ sigmoid æ¿€æ´»ç¡®ä¿ lambda_k âˆˆ [0, 1]
    self.lambda_k_logit = nn.Parameter(torch.zeros(1))
```

### é”™è¯¯å¤„ç†

- âœ… å‚æ•°éªŒè¯ï¼šFedSDG ä»…æ”¯æŒ ViT æ¨¡å‹
- âœ… ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿ state_dict é”®åŒ¹é…
- âœ… è¾¹ç•Œæ¡ä»¶ï¼šé¦–æ¬¡è®­ç»ƒå®¢æˆ·ç«¯çš„ç§æœ‰å‚æ•°å¤„ç†

---

## ğŸ“ æ–‡ä»¶ä¿®æ”¹æ¸…å•

### ä¿®æ”¹çš„æ–‡ä»¶

| æ–‡ä»¶ | ä¿®æ”¹è¡Œæ•° | ä¸»è¦æ”¹åŠ¨ |
|------|---------|---------|
| `src/models.py` | ~80 è¡Œ | LoRALayer æ‰©å±•ã€å‚æ•°è¿‡æ»¤ã€æ³¨å…¥å‡½æ•° |
| `src/federated_main.py` | ~60 è¡Œ | ç§æœ‰çŠ¶æ€ç®¡ç†ã€èšåˆé€»è¾‘ |
| `src/options.py` | ~5 è¡Œ | ç®—æ³•é€‰é¡¹ã€éªŒè¯é€»è¾‘ |
| `src/utils.py` | ~20 è¡Œ | é€šä¿¡ç»Ÿè®¡ã€æ˜¾ç¤ºå‡½æ•° |

### æ–°å¢çš„æ–‡ä»¶

| æ–‡ä»¶ | è¡Œæ•° | ç”¨é€” |
|------|------|------|
| `src/test_fedsdg.py` | 450 è¡Œ | å•å…ƒæµ‹è¯•è„šæœ¬ |
| `src/run_fedsdg_test.sh` | 40 è¡Œ | é›†æˆæµ‹è¯•è„šæœ¬ |
| `FedSDGå®æ–½æŠ€æœ¯æŠ¥å‘Š.md` | æœ¬æ–‡ä»¶ | æŠ€æœ¯æ–‡æ¡£ |

**æ€»è®¡**: ~165 è¡Œæ ¸å¿ƒä»£ç ä¿®æ”¹ï¼Œ~490 è¡Œæµ‹è¯•ä»£ç 

---

## ğŸš€ ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ç”¨æ³•

```bash
# FedSDG è®­ç»ƒï¼ˆCIFAR-10, Î±=0.1ï¼‰
python3 federated_main.py \
    --alg fedsdg \
    --model vit \
    --dataset cifar \
    --num_classes 10 \
    --epochs 50 \
    --num_users 100 \
    --frac 0.1 \
    --local_ep 5 \
    --local_bs 32 \
    --lr 0.0001 \
    --lora_r 8 \
    --lora_alpha 16 \
    --dirichlet_alpha 0.1 \
    --gpu 0 \
    --log_subdir fedsdg_cifar10_alpha0.1
```

### é¢„è®­ç»ƒæ¨¡å‹ + ç¦»çº¿æ•°æ®

```bash
# FedSDG + é¢„è®­ç»ƒ ViT + CIFAR-100
python3 federated_main.py \
    --alg fedsdg \
    --model vit \
    --model_variant pretrained \
    --dataset cifar100 \
    --num_classes 100 \
    --image_size 224 \
    --use_offline_data \
    --offline_data_root ../data/preprocessed/ \
    --epochs 50 \
    --num_users 100 \
    --frac 0.1 \
    --local_ep 5 \
    --local_bs 16 \
    --lr 0.0001 \
    --lora_r 8 \
    --lora_alpha 16 \
    --dirichlet_alpha 0.1 \
    --gpu 0 \
    --log_subdir fedsdg_pretrained_vit_cifar100_alpha0.1
```

### å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--alg` | ç®—æ³•ç±»å‹ | `fedsdg` |
| `--lora_r` | LoRA ç§© | 8 (å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡) |
| `--lora_alpha` | LoRA ç¼©æ”¾å› å­ | 16 (æ ‡å‡†é…ç½®) |
| `--dirichlet_alpha` | Non-IID ç¨‹åº¦ | 0.1 (å¼ºå¼‚æ„) |
| `--lr` | å­¦ä¹ ç‡ | 0.0001 (é¢„è®­ç»ƒ), 0.001 (ä»é›¶è®­ç»ƒ) |

---

## ğŸ” å…³é”®æŠ€æœ¯ç»†èŠ‚

### 1. é—¨æ§å‚æ•°åˆå§‹åŒ–

```python
self.lambda_k_logit = nn.Parameter(torch.zeros(1))
```

- åˆå§‹åŒ–ä¸º 0ï¼Œç»è¿‡ sigmoid å Î»_k â‰ˆ 0.5
- è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜çš„å…¨å±€/ç§æœ‰æƒé‡
- ä¸åŒå®¢æˆ·ç«¯å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„ Î»_k å€¼

### 2. ç§æœ‰å‚æ•°å­˜å‚¨ç­–ç•¥

```python
private_state[name] = param.data.clone().cpu()
```

- ä¿å­˜åˆ° CPU ä»¥èŠ‚çœ GPU å†…å­˜
- ä½¿ç”¨ `.clone()` é¿å…å¼•ç”¨é—®é¢˜
- å­—å…¸é”®ä¸ºå®Œæ•´å‚æ•°åï¼ˆå¦‚ `'transformer.layers.0.self_attn.out_proj.lora_A_private'`ï¼‰

### 3. å‚æ•°è¿‡æ»¤æœºåˆ¶

```python
if '_private' in name or 'lambda_k' in name:
    continue  # è·³è¿‡ç§æœ‰å‚æ•°
```

- ç®€å•é«˜æ•ˆçš„å­—ç¬¦ä¸²åŒ¹é…
- ä¸ä¾èµ–å‚æ•°ä½ç½®æˆ–ç´¢å¼•
- æ˜“äºæ‰©å±•å’Œç»´æŠ¤

### 4. å‘åå…¼å®¹æ€§

æ‰€æœ‰ä¿®æ”¹éƒ½é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿è¯å‘åå…¼å®¹ï¼š
- æ–°å¢å‚æ•°é»˜è®¤å€¼ä¸º `False` æˆ– `None`
- ä½¿ç”¨ `if args.alg == 'fedsdg'` æ¡ä»¶åˆ†æ”¯
- ä¸ä¿®æ”¹ä»»ä½•ç°æœ‰å‡½æ•°çš„é»˜è®¤è¡Œä¸º

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

### é€šä¿¡æ•ˆç‡

| æŒ‡æ ‡ | FedAvg | FedLoRA | FedSDG |
|------|--------|---------|--------|
| å•è½®é€šä¿¡é‡ (åŒå‘) | 45.6 MB | 0.4 MB | **0.4 MB** |
| 50 è½®æ€»é€šä¿¡é‡ | 2.28 GB | 20 MB | **20 MB** |
| å‹ç¼©ç‡ | 100% | 0.87% | **0.87%** |
| èŠ‚çœç‡ | 0% | 99.13% | **99.13%** |

### Non-IID æ€§èƒ½ï¼ˆé¢„æœŸï¼‰

åœ¨ Î±=0.1 çš„å¼º Non-IID åœºæ™¯ä¸‹ï¼š
- **FedAvg**: åŸºå‡†æ€§èƒ½
- **FedLoRA**: å¯èƒ½å› ç¼ºä¹ä¸ªæ€§åŒ–è€Œæ€§èƒ½ä¸‹é™
- **FedSDG**: é€šè¿‡ç§æœ‰åˆ†æ”¯å­¦ä¹ å®¢æˆ·ç«¯ç‰¹å®šæ¨¡å¼ï¼Œé¢„æœŸæ€§èƒ½ä¼˜äº FedLoRA

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å†…å­˜å¼€é”€

FedSDG çš„å¯è®­ç»ƒå‚æ•°çº¦ä¸º FedLoRA çš„ **2å€**ï¼ˆå…¨å±€ + ç§æœ‰åˆ†æ”¯ï¼‰ï¼š
- FedLoRA: ~50K å‚æ•°
- FedSDG: ~100K å‚æ•°

ä½†ç›¸æ¯”å®Œæ•´æ¨¡å‹ï¼ˆ5.7Mï¼‰ä»ç„¶éå¸¸å°ã€‚

### 2. å®¢æˆ·ç«¯æ•°é‡

`local_private_states` å­—å…¸ä¼šä¸ºæ¯ä¸ª**æ›¾ç»å‚ä¸è®­ç»ƒ**çš„å®¢æˆ·ç«¯å­˜å‚¨ç§æœ‰å‚æ•°ï¼š
- 100 ä¸ªå®¢æˆ·ç«¯ Ã— 50K å‚æ•° Ã— 4 å­—èŠ‚ â‰ˆ 20 MB
- å»ºè®®å®šæœŸæ¸…ç†ä¸æ´»è·ƒå®¢æˆ·ç«¯çš„çŠ¶æ€

### 3. GPU å†…å­˜

ç§æœ‰å‚æ•°ä¿å­˜åˆ° CPUï¼Œä¸å ç”¨ GPU å†…å­˜ã€‚è®­ç»ƒæ—¶ä»…å½“å‰å®¢æˆ·ç«¯çš„ç§æœ‰å‚æ•°åœ¨ GPU ä¸Šã€‚

### 4. å…¼å®¹æ€§

- âœ… æ”¯æŒæ‰‹å†™ ViT å’Œ timm é¢„è®­ç»ƒ ViT
- âœ… æ”¯æŒ CIFAR-10 å’Œ CIFAR-100
- âœ… æ”¯æŒç¦»çº¿é¢„å¤„ç†æ•°æ®
- âŒ æš‚ä¸æ”¯æŒ CNN æ¨¡å‹ï¼ˆå¯æ‰©å±•ï¼‰

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1: é€šä¿¡é‡ä¸ä¸€è‡´

**ç—‡çŠ¶**: FedSDG çš„é€šä¿¡é‡å¤§äº FedLoRA

**åŸå› **: `get_lora_state_dict()` æœªæ­£ç¡®è¿‡æ»¤ç§æœ‰å‚æ•°

**è§£å†³**: æ£€æŸ¥å‚æ•°åæ˜¯å¦åŒ…å« `'_private'` æˆ– `'lambda_k'`

### é—®é¢˜ 2: ç§æœ‰å‚æ•°æœªæ›´æ–°

**ç—‡çŠ¶**: è®­ç»ƒè¿‡ç¨‹ä¸­ Î»_k å§‹ç»ˆä¸º 0.5

**åŸå› **: ç§æœ‰å‚æ•°æœªæ­£ç¡®åŠ è½½æˆ–ä¿å­˜

**è§£å†³**: æ£€æŸ¥ `local_private_states` å­—å…¸æ˜¯å¦æ­£ç¡®æ›´æ–°

### é—®é¢˜ 3: å†…å­˜æº¢å‡º

**ç—‡çŠ¶**: GPU å†…å­˜ä¸è¶³

**åŸå› **: ç§æœ‰å‚æ•°æœªä¿å­˜åˆ° CPU

**è§£å†³**: ç¡®ä¿ä½¿ç”¨ `.cpu()` ä¿å­˜ç§æœ‰å‚æ•°

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **LoRA**: Hu et al., "LoRA: Low-Rank Adaptation of Large Language Models", ICLR 2022
2. **FedAvg**: McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data", AISTATS 2017
3. **Non-IID Partitioning**: Hsu et al., "Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification", arXiv 2019

---

## ğŸ¯ æœªæ¥æ‰©å±•æ–¹å‘

### çŸ­æœŸï¼ˆå·²å®ç°ï¼‰
- âœ… åŸºç¡€ FedSDG ç®—æ³•å®ç°
- âœ… é€šä¿¡é‡ä¼˜åŒ–
- âœ… å®¢æˆ·ç«¯ç§æœ‰çŠ¶æ€ç®¡ç†
- âœ… å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

### ä¸­æœŸï¼ˆå¯æ‰©å±•ï¼‰
- ğŸ”² æ”¯æŒ CNN æ¨¡å‹ï¼ˆResNet, MobileNetï¼‰
- ğŸ”² è‡ªé€‚åº”é—¨æ§æœºåˆ¶ï¼ˆæ ¹æ®æ•°æ®åˆ†å¸ƒè‡ªåŠ¨è°ƒæ•´ Î»_kï¼‰
- ğŸ”² ç§æœ‰å‚æ•°å‹ç¼©ï¼ˆå‡å°‘å†…å­˜å¼€é”€ï¼‰
- ğŸ”² å¤šä»»åŠ¡å­¦ä¹ æ”¯æŒ

### é•¿æœŸï¼ˆç ”ç©¶æ–¹å‘ï¼‰
- ğŸ”² ç†è®ºåˆ†æï¼šæ”¶æ•›æ€§è¯æ˜
- ğŸ”² éšç§ä¿æŠ¤ï¼šå·®åˆ†éšç§ + FedSDG
- ğŸ”² å¼‚æ„è®¾å¤‡ï¼šå¤„ç†ä¸åŒè®¡ç®—èƒ½åŠ›çš„å®¢æˆ·ç«¯
- ğŸ”² åŠ¨æ€æ¶æ„ï¼šæ ¹æ®å®¢æˆ·ç«¯æ•°æ®é‡è°ƒæ•´ç§æœ‰åˆ†æ”¯å¤§å°

---

## ğŸ“ æ€»ç»“

### å®æ–½æˆæœ

1. **å®Œå…¨éä¾µå…¥å¼**: ä¸å½±å“ç°æœ‰ FedAvg å’Œ FedLoRA åŠŸèƒ½
2. **é€šä¿¡æ•ˆç‡**: ä¸ FedLoRA ä¿æŒå®Œå…¨ä¸€è‡´ï¼ˆ0.2MB/è½®ï¼‰
3. **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºç»´æŠ¤å’Œæ‰©å±•
4. **å®Œæ•´æµ‹è¯•**: å•å…ƒæµ‹è¯• + é›†æˆæµ‹è¯•è¦†ç›–

### æ ¸å¿ƒä¼˜åŠ¿

- âœ… **é€šä¿¡é«˜æ•ˆ**: ç§æœ‰å‚æ•°ä¸ä¸Šä¼ ï¼Œé€šä¿¡é‡ä¸ FedLoRA ä¸€è‡´
- âœ… **ä¸ªæ€§åŒ–å¼º**: æ¯ä¸ªå®¢æˆ·ç«¯ç»´æŠ¤ç‹¬ç«‹çš„ç§æœ‰åˆ†æ”¯
- âœ… **æ˜“äºä½¿ç”¨**: ä»…éœ€æ·»åŠ  `--alg fedsdg` å‚æ•°
- âœ… **å¯æ‰©å±•æ€§**: æ”¯æŒé¢„è®­ç»ƒæ¨¡å‹å’Œç¦»çº¿æ•°æ®

### æŠ€æœ¯äº®ç‚¹

1. **åŒè·¯æ¶æ„**: å…¨å±€åˆ†æ”¯ï¼ˆèšåˆï¼‰+ ç§æœ‰åˆ†æ”¯ï¼ˆæœ¬åœ°ï¼‰
2. **é—¨æ§æœºåˆ¶**: å¯å­¦ä¹ çš„ Î»_k è‡ªåŠ¨å¹³è¡¡å…¨å±€/ç§æœ‰æƒé‡
3. **çŠ¶æ€ç®¡ç†**: é«˜æ•ˆçš„å®¢æˆ·ç«¯ç§æœ‰å‚æ•°å­˜å‚¨å’ŒåŠ è½½
4. **å‚æ•°è¿‡æ»¤**: è‡ªåŠ¨è¿‡æ»¤ç§æœ‰å‚æ•°ï¼Œç¡®ä¿é€šä¿¡æ•ˆç‡

---

## ğŸ‘¥ è´¡çŒ®è€…

- **å®æ–½è€…**: Cascade AI
- **å®¡æ ¸è€…**: å¾…å®š
- **æµ‹è¯•è€…**: å¾…å®š

---

## ğŸ“„ è®¸å¯è¯

æœ¬å®æ–½éµå¾ªé¡¹ç›®åŸæœ‰è®¸å¯è¯ã€‚

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2026-01-06  
**ç‰ˆæœ¬**: v1.0  
**çŠ¶æ€**: âœ… å®æ–½å®Œæˆï¼Œå¾…æµ‹è¯•éªŒè¯


python3 federated_main.py \
    --alg fedsdg \
    --model vit \
    --dataset cifar \
    --epochs 50 \
    --num_users 100 \
    --frac 0.1 \
    --lora_r 8 \
    --lora_alpha 16 \
    --dirichlet_alpha 0.1 \
    --gpu 0


python3 federated_main.py \
    --alg fedsdg \
    --model vit \
    --model_variant pretrained \
    --dataset cifar100 \
    --image_size 224 \
    --use_offline_data \
    --epochs 50 \
    --lora_r 8 \
    --gpu 0