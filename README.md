# Federated Learning PyTorch

A modular and extensible federated learning framework supporting multiple algorithms:

- **FedAvg**: Federated Averaging (baseline)
- **FedLoRA**: Federated Low-Rank Adaptation (parameter-efficient)
- **FedSDG**: Federated Structure-Decoupled Gating (dual-path personalization)

## ğŸ“ Project Structure

```
Federated-Learning-PyTorch/
â”œâ”€â”€ fl/                           # Core federated learning library
â”‚   â”œâ”€â”€ algorithms/               # Aggregation algorithms
â”‚   â”‚   â”œâ”€â”€ fedavg.py            # FedAvg aggregation
â”‚   â”‚   â””â”€â”€ fedlora.py           # FedLoRA/FedSDG aggregation
â”‚   â”œâ”€â”€ clients/                  # Client-side components
â”‚   â”‚   â””â”€â”€ local_trainer.py     # Local training logic
â”‚   â”œâ”€â”€ data/                     # Data processing
â”‚   â”‚   â”œâ”€â”€ datasets.py          # Dataset loading
â”‚   â”‚   â”œâ”€â”€ sampling.py          # Dirichlet partitioning
â”‚   â”‚   â””â”€â”€ offline_dataset.py   # Offline preprocessed datasets
â”‚   â”œâ”€â”€ models/                   # Model definitions
â”‚   â”‚   â”œâ”€â”€ cnn.py               # CNN models
â”‚   â”‚   â”œâ”€â”€ mlp.py               # MLP model
â”‚   â”‚   â”œâ”€â”€ vit.py               # Vision Transformer
â”‚   â”‚   â””â”€â”€ lora.py              # LoRA implementation
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ paths.py             # Path management
â”‚       â”œâ”€â”€ checkpoint.py        # Checkpoint management
â”‚       â”œâ”€â”€ communication.py     # Communication statistics
â”‚       â”œâ”€â”€ evaluation.py        # Evaluation functions
â”‚       â””â”€â”€ logger.py            # Logging utilities
â”‚
â”œâ”€â”€ data/                         # Datasets
â”‚   â”œâ”€â”€ cifar/                   # CIFAR-10
â”‚   â”œâ”€â”€ cifar100/                # CIFAR-100
â”‚   â”œâ”€â”€ mnist/                   # MNIST
â”‚   â””â”€â”€ preprocessed/            # Offline preprocessed data
â”‚
â”œâ”€â”€ scripts/                      # Executable scripts
â”‚   â”œâ”€â”€ train/                   # Training scripts
â”‚   â”‚   â”œâ”€â”€ run_fedavg_cifar.sh
â”‚   â”‚   â”œâ”€â”€ run_fedlora_cifar100.sh
â”‚   â”‚   â””â”€â”€ run_fedsdg_cifar100.sh
â”‚   â”œâ”€â”€ preprocess/              # Data preprocessing
â”‚   â””â”€â”€ analysis/                # Analysis tools
â”‚
â”œâ”€â”€ logs/                         # TensorBoard logs
â”œâ”€â”€ save/                         # Saved models and results
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ objects/
â”‚   â””â”€â”€ summaries/
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ algorithms/              # Algorithm documentation
â”‚   â”œâ”€â”€ user_guides/             # User guides
â”‚   â””â”€â”€ technical_reports/       # Technical reports
â”‚
â”œâ”€â”€ main.py                       # Main entry point
â”œâ”€â”€ options.py                    # Argument parser
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/your-repo/Federated-Learning-PyTorch.git
cd Federated-Learning-PyTorch

# Create virtual environment (optional but recommended)
conda create -n fl python=3.10
conda activate fl

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Experiments

#### FedAvg (Baseline)
```bash
# Using script
bash scripts/train/run_fedavg_cifar.sh

# Or direct command
python main.py --alg fedavg --model cnn --dataset cifar --epochs 100 --gpu 0
```

#### FedLoRA (Parameter-Efficient)
```bash
# Using script
bash scripts/train/run_fedlora_cifar100.sh

# Or direct command
python main.py \
    --alg fedlora \
    --model vit \
    --model_variant pretrained \
    --dataset cifar100 \
    --use_offline_data \
    --offline_data_root ./data/preprocessed/ \
    --epochs 100 \
    --lora_r 8 \
    --lora_alpha 16 \
    --gpu 0
```

#### FedSDG (Dual-Path Personalization)
```bash
# Using script
bash scripts/train/run_fedsdg_cifar100.sh

# Or direct command
python main.py \
    --alg fedsdg \
    --model vit \
    --model_variant pretrained \
    --dataset cifar100 \
    --use_offline_data \
    --epochs 100 \
    --lora_r 8 \
    --lambda1 0.01 \
    --lambda2 0.0001 \
    --server_agg_method fedavg \
    --gpu 0
```

### 3. Monitor Training

```bash
tensorboard --logdir=./logs
```

## ğŸ“Š Supported Algorithms

| Algorithm | Description | Communication Efficiency |
|-----------|-------------|-------------------------|
| FedAvg | Standard federated averaging | Baseline (100%) |
| FedLoRA | Low-rank adaptation | ~3.5% of full model |
| FedSDG | Dual-path with gating | ~3.5% of full model |

## ğŸ”§ Key Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--alg` | Algorithm: fedavg, fedlora, fedsdg | fedavg |
| `--model` | Model: mlp, cnn, vit | mlp |
| `--model_variant` | scratch or pretrained (ViT only) | scratch |
| `--dataset` | Dataset: mnist, cifar, cifar100 | mnist |
| `--epochs` | Number of communication rounds | 10 |
| `--num_users` | Number of clients | 100 |
| `--frac` | Client participation fraction | 0.1 |
| `--local_ep` | Local training epochs | 10 |
| `--local_bs` | Local batch size | 10 |
| `--lr` | Learning rate | 0.01 |
| `--dirichlet_alpha` | Non-IID parameter (smaller=more heterogeneous) | 0.5 |
| `--lora_r` | LoRA rank (FedLoRA/FedSDG) | 8 |
| `--lora_alpha` | LoRA scaling factor | 16 |
| `--use_offline_data` | Use preprocessed data | False |
| `--gpu` | GPU ID (-1 for CPU) | -1 |

## ğŸ“ Data Preparation

### Online Mode (Default)
Datasets are automatically downloaded when first used.

### Offline Mode (Recommended for large datasets)
```bash
# Preprocess CIFAR-100 to 224x224
python src/preprocess_cifar100.py --image_size 224

# Use with offline data
python main.py --use_offline_data --offline_data_root ./data/preprocessed/ ...
```

## ğŸ“ˆ Output Files

After training, find results in:

- **TensorBoard logs**: `./logs/<experiment_name>/`
- **Training summary**: `./save/summaries/<experiment>_summary.txt`
- **Saved models**: `./save/models/`
- **Training objects**: `./save/objects/`
- **Checkpoints**: `./save/checkpoints/`

## ğŸ“š Documentation

See the `docs/` directory for detailed documentation:

- [Algorithm Design](docs/algorithms/) - FedAvg, FedLoRA, FedSDG design docs
- [User Guides](docs/user_guides/) - Data preprocessing, pretrained models
- [Technical Reports](docs/technical_reports/) - Bug reports, optimization

## ğŸ”— Legacy Support

The original source files in `src/` are preserved for backward compatibility.
New code should use the modular structure in `fl/`.

## ğŸ“ Citation

If you use this code, please cite:

```bibtex
@misc{federated-learning-pytorch,
  title={Federated Learning PyTorch},
  author={FL Research Team},
  year={2024},
  url={https://github.com/your-repo/Federated-Learning-PyTorch}
}
```

## ğŸ“„ License

MIT License


